{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.callbacks import EarlyStopping , ReduceLROnPlateau , ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "sns.set(color_codes=True)\n",
    "#sns.set_color_codes()\n",
    "\n",
    "pd.options.display.max_rows = 15\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network structure. \n",
    "# not to include the input layer\n",
    "net_layers = (3,2)\n",
    "\n",
    "epochs=200\n",
    "batch_size=300\n",
    "\n",
    "learning_rate = 1e-3\n",
    "decay = learning_rate / epochs\n",
    "\n",
    "patience=80\n",
    "\n",
    "test_split = 0.1\n",
    "validation_split = 0.2\n",
    "\n",
    "# trials\n",
    "# 11 R2-> -0.12\n",
    "# 11,16,8,4 R2-> 0.37\n",
    "# 11,11,6 R2-> -0.2\n",
    "# 11,8,6 R2->   0.316\n",
    "# 11,11,8,4 R2->  0.37\n",
    "# 11, 6 R2-> 0.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- read data file\n",
    "# 1- read processed file\n",
    "file_dir = '../data/processed-data/'\n",
    "data_file = 'normalized_dataset.csv'\n",
    "#data_file = 'standardized_normalized_dataset.csv'\n",
    "data = pd.read_csv(file_dir + data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ltcy</th>\n",
       "      <th>svc_cpu_use</th>\n",
       "      <th>svc_cpu_thr</th>\n",
       "      <th>svc_net_use</th>\n",
       "      <th>svc_disk_use</th>\n",
       "      <th>system_cpu_use</th>\n",
       "      <th>system_cpu_sat</th>\n",
       "      <th>system_net_use</th>\n",
       "      <th>svc_req_size</th>\n",
       "      <th>svc_resp_size</th>\n",
       "      <th>svc_pods</th>\n",
       "      <th>svc_req_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.332</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.037</td>\n",
       "      <td>16.982</td>\n",
       "      <td>1.591</td>\n",
       "      <td>3.206</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.012</td>\n",
       "      <td>7.000</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.047</td>\n",
       "      <td>20.583</td>\n",
       "      <td>1.608</td>\n",
       "      <td>3.552</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.019</td>\n",
       "      <td>7.000</td>\n",
       "      <td>1.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.469</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.039</td>\n",
       "      <td>19.448</td>\n",
       "      <td>1.390</td>\n",
       "      <td>3.586</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.024</td>\n",
       "      <td>7.000</td>\n",
       "      <td>2.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.490</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.108</td>\n",
       "      <td>17.319</td>\n",
       "      <td>1.730</td>\n",
       "      <td>3.512</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.022</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.133</td>\n",
       "      <td>16.650</td>\n",
       "      <td>1.917</td>\n",
       "      <td>3.449</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.023</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ltcy  svc_cpu_use  svc_cpu_thr  svc_net_use  svc_disk_use  system_cpu_use  \\\n",
       "0 0.332        0.557        0.332        0.325         0.037          16.982   \n",
       "1 0.400        0.616        0.300        0.351         0.047          20.583   \n",
       "2 0.469        0.608        0.316        0.362         0.039          19.448   \n",
       "3 0.490        0.624        0.300        0.362         0.108          17.319   \n",
       "4 0.500        0.608        0.316        0.374         0.133          16.650   \n",
       "\n",
       "   system_cpu_sat  system_net_use  svc_req_size  svc_resp_size  svc_pods  \\\n",
       "0           1.591           3.206         0.002          0.012     7.000   \n",
       "1           1.608           3.552         0.003          0.019     7.000   \n",
       "2           1.390           3.586         0.003          0.024     7.000   \n",
       "3           1.730           3.512         0.003          0.022     6.000   \n",
       "4           1.917           3.449         0.003          0.023     3.000   \n",
       "\n",
       "   svc_req_rate  \n",
       "0         0.980  \n",
       "1         1.620  \n",
       "2         2.180  \n",
       "3         2.130  \n",
       "4         2.220  "
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1584, 12)"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 11 features\n"
     ]
    }
   ],
   "source": [
    "targets = data['ltcy']\n",
    "inputs = data.drop(['ltcy'], axis=1)\n",
    "\n",
    "n_features = inputs.values.shape[1]\n",
    "print(\"there are {} features\".format(n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 features selected\n",
      "columns selected are svc_cpu_thr, system_cpu_use, svc_req_rate\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    sfm = SelectFromModel(\n",
    "                RandomForestRegressor(n_jobs=-1, max_depth=10, n_estimators=15),\n",
    "                threshold='0.8*mean')\n",
    "\n",
    "    selectedFeatures = sfm.fit(inputs, targets).transform(inputs)\n",
    "    print('{} features selected'.format(selectedFeatures[1].shape[0]))\n",
    "\n",
    "    feature_list = inputs.columns[sfm.get_support()]\n",
    "    features = ''\n",
    "    features = ', '.join(feature_list)\n",
    "    \n",
    "    print(\"columns selected are {}\".format(features))\n",
    "\n",
    "    inputs = inputs[feature_list]\n",
    "    inputs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = selectedFeatures[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (1425,) , y_test (159,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(inputs, targets, test_size=test_split, shuffle=True, random_state=365)\n",
    "\n",
    "print(\"y_train {} , y_test {}\".format(y_train.shape, y_test.shape))\n",
    "\n",
    "# for better convergence and result scale target to values between 0 - 1\n",
    "y_train_max = y_train.max()\n",
    "y_test_max = y_test.max()\n",
    "\n",
    "y_train = y_train / y_train_max\n",
    "y_test = y_test / y_test_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)  # fit on training data only\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(nodes = net_layers):   # this does not work with KerasRegressor. interesting\n",
    "    # create model\n",
    "    model = Sequential()  \n",
    "    \n",
    "    #if isinstance(nodes, int):\n",
    "    if not nodes:\n",
    "        model.add(Dense(1, input_dim=n_inputs, kernel_initializer='normal'))\n",
    "    \n",
    "    else:\n",
    "        model.add(Dense(nodes[1], input_dim=n_inputs, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "        layer = 0\n",
    "        while layer < len(nodes):\n",
    "            model.add(Dense(nodes[layer], kernel_initializer='normal', activation='relu'))\n",
    "            layer = layer + 1\n",
    "        \n",
    "        #model.add(Dense(1, kernel_initializer='normal', activation='linear')) \n",
    "        model.add(Dense(1, kernel_initializer='normal'))  \n",
    "    \n",
    "    adam = Adam(lr=learning_rate, decay=decay)\n",
    "    \n",
    "    # or loss= 'mean_absolute_percentage_error'\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mse'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1140 samples, validate on 285 samples\n",
      "Epoch 1/200\n",
      "1140/1140 [==============================] - 6s 5ms/step - loss: 0.4283 - mean_squared_error: 0.4283 - val_loss: 0.4395 - val_mean_squared_error: 0.4395\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43954, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 2/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.4230 - mean_squared_error: 0.4230 - val_loss: 0.4342 - val_mean_squared_error: 0.4342\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43954 to 0.43423, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 3/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.4178 - mean_squared_error: 0.4178 - val_loss: 0.4289 - val_mean_squared_error: 0.4289\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43423 to 0.42890, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 4/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.4125 - mean_squared_error: 0.4125 - val_loss: 0.4235 - val_mean_squared_error: 0.4235\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.42890 to 0.42355, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 5/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.4072 - mean_squared_error: 0.4072 - val_loss: 0.4182 - val_mean_squared_error: 0.4182\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42355 to 0.41815, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 6/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.4019 - mean_squared_error: 0.4019 - val_loss: 0.4127 - val_mean_squared_error: 0.4127\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.41815 to 0.41271, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 7/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.3965 - mean_squared_error: 0.3965 - val_loss: 0.4072 - val_mean_squared_error: 0.4072\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.41271 to 0.40720, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 8/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3911 - mean_squared_error: 0.3911 - val_loss: 0.4016 - val_mean_squared_error: 0.4016\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.40720 to 0.40161, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 9/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3855 - mean_squared_error: 0.3855 - val_loss: 0.3959 - val_mean_squared_error: 0.3959\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.40161 to 0.39591, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 10/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.3799 - mean_squared_error: 0.3799 - val_loss: 0.3901 - val_mean_squared_error: 0.3901\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.39591 to 0.39008, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 11/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.3741 - mean_squared_error: 0.3741 - val_loss: 0.3841 - val_mean_squared_error: 0.3841\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.39008 to 0.38409, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 12/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3681 - mean_squared_error: 0.3681 - val_loss: 0.3779 - val_mean_squared_error: 0.3779\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.38409 to 0.37787, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 13/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.3619 - mean_squared_error: 0.3619 - val_loss: 0.3714 - val_mean_squared_error: 0.3714\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.37787 to 0.37139, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 14/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3554 - mean_squared_error: 0.3554 - val_loss: 0.3646 - val_mean_squared_error: 0.3646\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.37139 to 0.36456, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 15/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.3485 - mean_squared_error: 0.3485 - val_loss: 0.3573 - val_mean_squared_error: 0.3573\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.36456 to 0.35726, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 16/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.3411 - mean_squared_error: 0.3411 - val_loss: 0.3494 - val_mean_squared_error: 0.3494\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.35726 to 0.34938, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 17/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.3332 - mean_squared_error: 0.3332 - val_loss: 0.3408 - val_mean_squared_error: 0.3408\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.34938 to 0.34082, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 18/200\n",
      "1140/1140 [==============================] - 0s 22us/step - loss: 0.3245 - mean_squared_error: 0.3245 - val_loss: 0.3314 - val_mean_squared_error: 0.3314\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.34082 to 0.33143, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 19/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3149 - mean_squared_error: 0.3149 - val_loss: 0.3210 - val_mean_squared_error: 0.3210\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.33143 to 0.32104, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 20/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.3043 - mean_squared_error: 0.3043 - val_loss: 0.3095 - val_mean_squared_error: 0.3095\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.32104 to 0.30946, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 21/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.2924 - mean_squared_error: 0.2924 - val_loss: 0.2965 - val_mean_squared_error: 0.2965\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.30946 to 0.29649, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 22/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.2791 - mean_squared_error: 0.2791 - val_loss: 0.2819 - val_mean_squared_error: 0.2819\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.29649 to 0.28192, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 23/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.2642 - mean_squared_error: 0.2642 - val_loss: 0.2655 - val_mean_squared_error: 0.2655\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.28192 to 0.26555, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 24/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.2474 - mean_squared_error: 0.2474 - val_loss: 0.2472 - val_mean_squared_error: 0.2472\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.26555 to 0.24717, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 25/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2286 - mean_squared_error: 0.2286 - val_loss: 0.2267 - val_mean_squared_error: 0.2267\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.24717 to 0.22666, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 26/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.2077 - mean_squared_error: 0.2077 - val_loss: 0.2040 - val_mean_squared_error: 0.2040\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.22666 to 0.20398, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 27/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.1848 - mean_squared_error: 0.1848 - val_loss: 0.1793 - val_mean_squared_error: 0.1793\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.20398 to 0.17928, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 28/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1600 - mean_squared_error: 0.1600 - val_loss: 0.1530 - val_mean_squared_error: 0.1530\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.17928 to 0.15295, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 29/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.1339 - mean_squared_error: 0.1339 - val_loss: 0.1257 - val_mean_squared_error: 0.1257\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.15295 to 0.12571, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 30/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.1073 - mean_squared_error: 0.1073 - val_loss: 0.0987 - val_mean_squared_error: 0.0987\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.12571 to 0.09869, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 31/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0816 - mean_squared_error: 0.0816 - val_loss: 0.0735 - val_mean_squared_error: 0.0735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss improved from 0.09869 to 0.07347, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 32/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0584 - mean_squared_error: 0.0584 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.07347 to 0.05200, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 33/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.05200 to 0.03620, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 34/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0272 - val_mean_squared_error: 0.0272\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.03620 to 0.02721, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 35/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.02721 to 0.02431, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 36/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 37/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 38/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02431\n",
      "Epoch 39/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.02431\n",
      "Epoch 40/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.02431\n",
      "Epoch 41/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.02431\n",
      "Epoch 42/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0242 - val_mean_squared_error: 0.0242\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.02431 to 0.02420, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 43/200\n",
      "1140/1140 [==============================] - 0s 22us/step - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0241 - val_mean_squared_error: 0.0241\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.02420 to 0.02406, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 44/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0239 - val_mean_squared_error: 0.0239\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.02406 to 0.02393, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 45/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0238 - val_mean_squared_error: 0.0238\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.02393 to 0.02382, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 46/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0237 - val_mean_squared_error: 0.0237\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.02382 to 0.02371, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 47/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0236 - val_mean_squared_error: 0.0236\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.02371 to 0.02361, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 48/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0235 - val_mean_squared_error: 0.0235\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.02361 to 0.02353, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 49/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0235 - val_mean_squared_error: 0.0235\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.02353 to 0.02345, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 50/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0234 - val_mean_squared_error: 0.0234\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.02345 to 0.02338, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 51/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0233 - val_mean_squared_error: 0.0233\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.02338 to 0.02332, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 52/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0233 - val_mean_squared_error: 0.0233\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.02332 to 0.02326, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 53/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0232 - val_mean_squared_error: 0.0232\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.02326 to 0.02321, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 54/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0232 - val_mean_squared_error: 0.0232\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.02321 to 0.02316, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 55/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0231 - val_mean_squared_error: 0.0231\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.02316 to 0.02312, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 56/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0231 - val_mean_squared_error: 0.0231\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.02312 to 0.02308, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 57/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0230 - val_mean_squared_error: 0.0230\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.02308 to 0.02304, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 58/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0230 - val_mean_squared_error: 0.0230\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.02304 to 0.02300, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 59/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0230 - val_mean_squared_error: 0.0230\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.02300 to 0.02296, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 60/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0229 - val_mean_squared_error: 0.0229\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.02296 to 0.02293, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 61/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0229 - val_mean_squared_error: 0.0229\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.02293 to 0.02289, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 62/200\n",
      "1140/1140 [==============================] - 0s 26us/step - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0229 - val_mean_squared_error: 0.0229\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.02289 to 0.02286, saving model to ../models/best_mlp_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0228 - val_mean_squared_error: 0.0228\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.02286 to 0.02283, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 64/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0228 - val_mean_squared_error: 0.0228\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.02283 to 0.02280, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 65/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0228 - val_mean_squared_error: 0.0228\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.02280 to 0.02276, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 66/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0227 - val_mean_squared_error: 0.0227\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.02276 to 0.02273, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 67/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0227 - val_mean_squared_error: 0.0227\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.02273 to 0.02270, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 68/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0227 - val_mean_squared_error: 0.0227\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.02270 to 0.02267, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 69/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.02267 to 0.02264, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 70/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.02264 to 0.02261, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 71/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.02261 to 0.02258, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 72/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0225 - val_mean_squared_error: 0.0225\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.02258 to 0.02255, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 73/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0225 - val_mean_squared_error: 0.0225\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.02255 to 0.02251, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 74/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0225 - val_mean_squared_error: 0.0225\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.02251 to 0.02248, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 75/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0225 - val_mean_squared_error: 0.0225\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.02248 to 0.02245, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 76/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0224 - val_mean_squared_error: 0.0224\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.02245 to 0.02242, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 77/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0224 - val_mean_squared_error: 0.0224\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.02242 to 0.02239, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 78/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0224 - val_mean_squared_error: 0.0224\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.02239 to 0.02236, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 79/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0223 - val_mean_squared_error: 0.0223\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.02236 to 0.02233, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 80/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0223 - val_mean_squared_error: 0.0223\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.02233 to 0.02230, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 81/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0223 - val_mean_squared_error: 0.0223\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.02230 to 0.02226, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 82/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0222 - val_mean_squared_error: 0.0222\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.02226 to 0.02223, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 83/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0222 - val_mean_squared_error: 0.0222\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.02223 to 0.02220, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 84/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0222 - val_mean_squared_error: 0.0222\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.02220 to 0.02217, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 85/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0221 - val_mean_squared_error: 0.0221\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.02217 to 0.02214, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 86/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0221 - val_mean_squared_error: 0.0221\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.02214 to 0.02211, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 87/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0221 - val_mean_squared_error: 0.0221\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.02211 to 0.02208, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 88/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0220 - val_mean_squared_error: 0.0220\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.02208 to 0.02204, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 89/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0220 - val_mean_squared_error: 0.0220\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.02204 to 0.02201, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 90/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0220 - val_mean_squared_error: 0.0220\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.02201 to 0.02198, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 91/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0220 - val_mean_squared_error: 0.0220\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.02198 to 0.02195, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 92/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0219 - val_mean_squared_error: 0.0219\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.02195 to 0.02192, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 93/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0219 - val_mean_squared_error: 0.0219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00093: val_loss improved from 0.02192 to 0.02189, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 94/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0219 - val_mean_squared_error: 0.0219\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.02189 to 0.02186, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 95/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0218 - val_mean_squared_error: 0.0218\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.02186 to 0.02183, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 96/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0218 - val_mean_squared_error: 0.0218\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.02183 to 0.02180, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 97/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0218 - val_mean_squared_error: 0.0218\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.02180 to 0.02177, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 98/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0217 - val_mean_squared_error: 0.0217\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.02177 to 0.02174, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 99/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0217 - val_mean_squared_error: 0.0217\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.02174 to 0.02171, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 100/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0217 - val_mean_squared_error: 0.0217\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.02171 to 0.02167, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 101/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0216 - val_mean_squared_error: 0.0216\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.02167 to 0.02164, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 102/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0216 - val_mean_squared_error: 0.0216\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.02164 to 0.02161, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 103/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0216 - val_mean_squared_error: 0.0216\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.02161 to 0.02159, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 104/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0216 - val_mean_squared_error: 0.0216\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.02159 to 0.02156, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 105/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0215 - val_mean_squared_error: 0.0215\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.02156 to 0.02153, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 106/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0215 - val_mean_squared_error: 0.0215\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.02153 to 0.02150, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 107/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0215 - val_mean_squared_error: 0.0215\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.02150 to 0.02147, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 108/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0214 - val_mean_squared_error: 0.0214\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.02147 to 0.02144, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 109/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0214 - val_mean_squared_error: 0.0214\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.02144 to 0.02141, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 110/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0214 - val_mean_squared_error: 0.0214\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.02141 to 0.02138, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 111/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0213 - val_mean_squared_error: 0.0213\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.02138 to 0.02135, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 112/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0213 - val_mean_squared_error: 0.0213\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.02135 to 0.02132, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 113/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0213 - val_mean_squared_error: 0.0213\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.02132 to 0.02129, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 114/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0213 - val_mean_squared_error: 0.0213\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.02129 to 0.02126, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 115/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0212 - val_mean_squared_error: 0.0212\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.02126 to 0.02123, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 116/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0212 - val_mean_squared_error: 0.0212\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.02123 to 0.02120, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 117/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0212 - val_mean_squared_error: 0.0212\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.02120 to 0.02117, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 118/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0211 - val_mean_squared_error: 0.0211\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.02117 to 0.02114, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 119/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0211 - val_mean_squared_error: 0.0211\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.02114 to 0.02111, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 120/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0211 - val_mean_squared_error: 0.0211\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.02111 to 0.02108, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 121/200\n",
      "1140/1140 [==============================] - 0s 22us/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0211 - val_mean_squared_error: 0.0211\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.02108 to 0.02106, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 122/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0210 - val_mean_squared_error: 0.0210\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.02106 to 0.02103, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 123/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0210 - val_mean_squared_error: 0.0210\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.02103 to 0.02100, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 124/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0210 - val_mean_squared_error: 0.0210\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.02100 to 0.02097, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 125/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0209 - val_mean_squared_error: 0.0209\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.02097 to 0.02094, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 126/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0209 - val_mean_squared_error: 0.0209\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.02094 to 0.02091, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 127/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0209 - val_mean_squared_error: 0.0209\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.02091 to 0.02089, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 128/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0209 - val_mean_squared_error: 0.0209\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.02089 to 0.02086, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 129/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0208 - val_mean_squared_error: 0.0208\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.02086 to 0.02083, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 130/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0208 - val_mean_squared_error: 0.0208\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.02083 to 0.02080, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 131/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0208 - val_mean_squared_error: 0.0208\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.02080 to 0.02077, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 132/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.02077 to 0.02075, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 133/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.02075 to 0.02072, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 134/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.02072 to 0.02069, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 135/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.02069 to 0.02067, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 136/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0206 - val_mean_squared_error: 0.0206\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.02067 to 0.02064, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 137/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0206 - val_mean_squared_error: 0.0206\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.02064 to 0.02061, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 138/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0206 - val_mean_squared_error: 0.0206\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.02061 to 0.02059, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 139/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0206 - val_mean_squared_error: 0.0206\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.02059 to 0.02056, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 140/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0205 - val_mean_squared_error: 0.0205\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.02056 to 0.02053, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 141/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0205 - val_mean_squared_error: 0.0205\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.02053 to 0.02051, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 142/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0205 - val_mean_squared_error: 0.0205\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.02051 to 0.02048, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 143/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0205 - val_mean_squared_error: 0.0205\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.02048 to 0.02045, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 144/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.02045 to 0.02043, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 145/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.02043 to 0.02040, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 146/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.02040 to 0.02038, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 147/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.02038 to 0.02035, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 148/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0203 - val_mean_squared_error: 0.0203\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.02035 to 0.02033, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 149/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0203 - val_mean_squared_error: 0.0203\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.02033 to 0.02030, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 150/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0203 - val_mean_squared_error: 0.0203\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.02030 to 0.02027, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 151/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0202 - val_mean_squared_error: 0.0202\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.02027 to 0.02025, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 152/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0202 - val_mean_squared_error: 0.0202\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.02025 to 0.02022, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 153/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0202 - val_mean_squared_error: 0.0202\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.02022 to 0.02020, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 154/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0202 - val_mean_squared_error: 0.0202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00154: val_loss improved from 0.02020 to 0.02017, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 155/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0201 - val_mean_squared_error: 0.0201\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.02017 to 0.02015, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 156/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0201 - val_mean_squared_error: 0.0201\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.02015 to 0.02013, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 157/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0201 - val_mean_squared_error: 0.0201\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.02013 to 0.02010, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 158/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0201 - val_mean_squared_error: 0.0201\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.02010 to 0.02008, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 159/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0201 - val_mean_squared_error: 0.0201\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.02008 to 0.02005, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 160/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0200 - val_mean_squared_error: 0.0200\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.02005 to 0.02003, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 161/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0200 - val_mean_squared_error: 0.0200\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.02003 to 0.02001, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 162/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0200 - val_mean_squared_error: 0.0200\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.02001 to 0.01998, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 163/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0200 - val_mean_squared_error: 0.0200\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.01998 to 0.01996, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 164/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0199 - val_mean_squared_error: 0.0199\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.01996 to 0.01994, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 165/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0199 - val_mean_squared_error: 0.0199\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.01994 to 0.01991, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 166/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0199 - val_mean_squared_error: 0.0199\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.01991 to 0.01989, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 167/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0199 - val_mean_squared_error: 0.0199\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.01989 to 0.01987, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 168/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0198 - val_mean_squared_error: 0.0198\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.01987 to 0.01984, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 169/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0198 - val_mean_squared_error: 0.0198\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.01984 to 0.01982, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 170/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0198 - val_mean_squared_error: 0.0198\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.01982 to 0.01980, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 171/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0198 - val_mean_squared_error: 0.0198\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.01980 to 0.01978, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 172/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0198 - val_mean_squared_error: 0.0198\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.01978 to 0.01976, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 173/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0197 - val_mean_squared_error: 0.0197\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.01976 to 0.01973, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 174/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0197 - val_mean_squared_error: 0.0197\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.01973 to 0.01971, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 175/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0197 - val_mean_squared_error: 0.0197\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.01971 to 0.01969, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 176/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0197 - val_mean_squared_error: 0.0197\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.01969 to 0.01967, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 177/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.01967 to 0.01965, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 178/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.01965 to 0.01963, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 179/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.01963 to 0.01960, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 180/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.01960 to 0.01958, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 181/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.01958 to 0.01956, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 182/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.01956 to 0.01954, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 183/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.01954 to 0.01952, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 184/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.01952 to 0.01950, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.01950 to 0.01948, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 186/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.01948 to 0.01946, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 187/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.01946 to 0.01944, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 188/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.01944 to 0.01942, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 189/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.01942 to 0.01940, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 190/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.01940 to 0.01938, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 191/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.01938 to 0.01936, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 192/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0193 - val_mean_squared_error: 0.0193\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.01936 to 0.01935, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 193/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0193 - val_mean_squared_error: 0.0193\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.01935 to 0.01933, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 194/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0193 - val_mean_squared_error: 0.0193\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.01933 to 0.01931, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 195/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0193 - val_mean_squared_error: 0.0193\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.01931 to 0.01929, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 196/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0193 - val_mean_squared_error: 0.0193\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.01929 to 0.01927, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 197/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0193 - val_mean_squared_error: 0.0193\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.01927 to 0.01925, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 198/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.01925 to 0.01923, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 199/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.01923 to 0.01921, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 200/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.01921 to 0.01919, saving model to ../models/best_mlp_model.h5\n",
      "Done..!\n"
     ]
    }
   ],
   "source": [
    "model = build_keras_model()\n",
    "\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "\n",
    "# save the best model only\n",
    "mc = ModelCheckpoint('../models/best_mlp_model.h5'\n",
    "                     , monitor='val_loss'\n",
    "                     , mode='min' \n",
    "                     , verbose=1\n",
    "                     #, save_weights_only=True\n",
    "                     , save_best_only=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)\n",
    "\n",
    "callbacks = [es, mc, reduce_lr]\n",
    "\n",
    "history = model.fit(x_train, y_train\n",
    "                    , epochs=epochs\n",
    "                    , batch_size=batch_size\n",
    "                    , verbose=1\n",
    "                    , validation_split=validation_split\n",
    "                    , shuffle=False\n",
    "                    , callbacks=callbacks)\n",
    "\n",
    "print (\"Done..!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Validation MSE mean: 0.07  std: (0.11)\n"
     ]
    }
   ],
   "source": [
    "val_loss = history.history['val_loss']\n",
    "val_loss_df = pd.DataFrame(val_loss)\n",
    "\n",
    "print(\"Model Validation MSE mean: %.2f  std: (%.2f)\" % \\\n",
    "      (val_loss_df.mean(), val_loss_df.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.015, Test MSE: 0.012\n"
     ]
    }
   ],
   "source": [
    "# load the saved model\n",
    "saved_model = load_model('../models/best_mlp_model.h5')\n",
    "\n",
    "# evaluate the model\n",
    "train_loss, train_mse = saved_model.evaluate(x_train, y_train, verbose=0)\n",
    "test_loss, test_mse = saved_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Train MSE: %.3f, Test MSE: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5735521 ],\n",
       "       [0.66887045],\n",
       "       [0.588026  ],\n",
       "       ...,\n",
       "       [0.70548165],\n",
       "       [0.6561152 ],\n",
       "       [0.5588804 ]], dtype=float32)"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regressor.fit(x_train, y_train)\n",
    "y_hat = saved_model.predict(x_train)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 (training) = 0.3030889998905718\n"
     ]
    }
   ],
   "source": [
    "r2_train = metrics.r2_score(y_train, y_hat)\n",
    "print('R2 (training) = {}'.format(r2_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_train, y_hat)\n",
    "plt.xlabel('Latency', size=18)\n",
    "plt.ylabel(\"predicted Latency\", size=18)\n",
    "#plt.xlim(-2,3)\n",
    "#plt.ylim(-3,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Residual PDF')"
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train = y_train.values\n",
    "sns.distplot(y_train - y_hat)\n",
    "plt.title('Residual PDF', size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Residual mean: 0.00 std: 0.18  min; -0.61 max: 0.51\n"
     ]
    }
   ],
   "source": [
    "# evaluation mean_absolute_percentage_error\n",
    "train_error =  y_train - y_hat\n",
    "train_error\n",
    "\n",
    "mean_error = np.mean(train_error)\n",
    "min_error = np.min(train_error)\n",
    "max_error = np.max(train_error)\n",
    "std_error = np.std(train_error)\n",
    "\n",
    "print(\"Train Residual mean: %.2f std: %.2f  min; %.2f max: %.2f\" \\\n",
    "      % (mean_error, std_error, min_error, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = saved_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 841,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test, y_hat_test)\n",
    "plt.xlabel('Latency', size=18)\n",
    "plt.ylabel('Predicted Latency', size=18)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.021</td>\n",
       "      <td>1.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.001</td>\n",
       "      <td>1.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.750</td>\n",
       "      <td>1.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.064</td>\n",
       "      <td>2.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.003</td>\n",
       "      <td>1.838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prediction  target\n",
       "0       2.021   1.785\n",
       "1       2.001   1.796\n",
       "2       1.750   1.697\n",
       "3       2.064   2.298\n",
       "4       2.003   1.838"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf = pd.DataFrame(np.exp(y_hat_test), columns=['prediction'])\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "perf['target'] = np.exp(y_test)\n",
    "perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>residual</th>\n",
       "      <th>difference%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.021</td>\n",
       "      <td>1.785</td>\n",
       "      <td>0.236</td>\n",
       "      <td>13.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.001</td>\n",
       "      <td>1.796</td>\n",
       "      <td>0.206</td>\n",
       "      <td>11.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.750</td>\n",
       "      <td>1.697</td>\n",
       "      <td>0.053</td>\n",
       "      <td>3.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.064</td>\n",
       "      <td>2.298</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>10.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.003</td>\n",
       "      <td>1.838</td>\n",
       "      <td>0.165</td>\n",
       "      <td>8.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2.324</td>\n",
       "      <td>2.269</td>\n",
       "      <td>0.055</td>\n",
       "      <td>2.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.758</td>\n",
       "      <td>1.506</td>\n",
       "      <td>0.252</td>\n",
       "      <td>16.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.882</td>\n",
       "      <td>2.043</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>7.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2.121</td>\n",
       "      <td>1.891</td>\n",
       "      <td>0.231</td>\n",
       "      <td>12.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2.246</td>\n",
       "      <td>2.230</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  target  residual  difference%\n",
       "0         2.021   1.785     0.236       13.209\n",
       "1         2.001   1.796     0.206       11.458\n",
       "2         1.750   1.697     0.053        3.136\n",
       "3         2.064   2.298    -0.234       10.167\n",
       "4         2.003   1.838     0.165        8.990\n",
       "..          ...     ...       ...          ...\n",
       "154       2.324   2.269     0.055        2.436\n",
       "155       1.758   1.506     0.252       16.712\n",
       "156       1.882   2.043    -0.161        7.877\n",
       "157       2.121   1.891     0.231       12.212\n",
       "158       2.246   2.230     0.016        0.696\n",
       "\n",
       "[159 rows x 4 columns]"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing mean_absolute_percentage_error\n",
    "perf['residual'] = perf['prediction'] - perf['target']\n",
    "perf['difference%'] = np.absolute(perf['residual'] * 100 / perf['target'])\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>residual</th>\n",
       "      <th>difference%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159.000</td>\n",
       "      <td>159.000</td>\n",
       "      <td>159.000</td>\n",
       "      <td>159.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.908</td>\n",
       "      <td>1.925</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>10.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.183</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.257</td>\n",
       "      <td>6.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.640</td>\n",
       "      <td>1.467</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.748</td>\n",
       "      <td>1.697</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>4.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.876</td>\n",
       "      <td>1.922</td>\n",
       "      <td>0.024</td>\n",
       "      <td>11.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.064</td>\n",
       "      <td>2.177</td>\n",
       "      <td>0.202</td>\n",
       "      <td>15.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.347</td>\n",
       "      <td>2.718</td>\n",
       "      <td>0.385</td>\n",
       "      <td>34.342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction  target  residual  difference%\n",
       "count     159.000 159.000   159.000      159.000\n",
       "mean        1.908   1.925    -0.017       10.615\n",
       "std         0.183   0.305     0.257        6.834\n",
       "min         1.640   1.467    -0.911        0.144\n",
       "25%         1.748   1.697    -0.172        4.927\n",
       "50%         1.876   1.922     0.024       11.025\n",
       "75%         2.064   2.177     0.202       15.648\n",
       "max         2.347   2.718     0.385       34.342"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>residual</th>\n",
       "      <th>difference%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1.97</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2.09</td>\n",
       "      <td>2.09</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.93</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2.01</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2.07</td>\n",
       "      <td>2.07</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.69</td>\n",
       "      <td>1.70</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2.14</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.77</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2.25</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2.01</td>\n",
       "      <td>2.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1.76</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.93</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.79</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2.10</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.79</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2.31</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>1.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1.69</td>\n",
       "      <td>1.72</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2.22</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2.18</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2.32</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2.05</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2.02</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2.18</td>\n",
       "      <td>2.26</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2.19</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2.29</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2.08</td>\n",
       "      <td>2.17</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2.26</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.05</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2.08</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.67</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>4.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2.10</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>4.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2.12</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>4.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2.17</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2.04</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2.35</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2.18</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2.02</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>5.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2.09</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>5.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2.16</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.15</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>5.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2.22</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.06</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>5.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.90</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>6.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.81</td>\n",
       "      <td>1.94</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>6.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>6.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>6.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2.12</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>7.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.99</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>7.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.04</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1.90</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>7.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.85</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.64</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>8.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.13</td>\n",
       "      <td>8.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.93</td>\n",
       "      <td>2.11</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>8.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.13</td>\n",
       "      <td>8.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.64</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.13</td>\n",
       "      <td>8.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.00</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.17</td>\n",
       "      <td>8.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>9.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.03</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>9.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.70</td>\n",
       "      <td>1.88</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>9.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>9.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.89</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1.85</td>\n",
       "      <td>2.05</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>10.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.06</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>10.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2.14</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.20</td>\n",
       "      <td>10.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2.07</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.20</td>\n",
       "      <td>10.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2.11</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.20</td>\n",
       "      <td>10.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.67</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.17</td>\n",
       "      <td>11.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.67</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.17</td>\n",
       "      <td>11.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.86</td>\n",
       "      <td>2.09</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>11.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.87</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>11.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.21</td>\n",
       "      <td>11.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1.68</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.17</td>\n",
       "      <td>11.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.69</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.18</td>\n",
       "      <td>11.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.87</td>\n",
       "      <td>2.11</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.68</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.18</td>\n",
       "      <td>11.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.16</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>11.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.74</td>\n",
       "      <td>1.97</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>11.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2.00</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.21</td>\n",
       "      <td>11.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.77</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2.06</td>\n",
       "      <td>2.35</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>12.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2.12</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.23</td>\n",
       "      <td>12.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2.03</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.22</td>\n",
       "      <td>12.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2.01</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.22</td>\n",
       "      <td>12.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2.15</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>12.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.69</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.95</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.22</td>\n",
       "      <td>12.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>12.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.14</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.24</td>\n",
       "      <td>12.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.02</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.24</td>\n",
       "      <td>13.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2.04</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.24</td>\n",
       "      <td>13.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.21</td>\n",
       "      <td>13.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.04</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.25</td>\n",
       "      <td>13.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.92</td>\n",
       "      <td>2.23</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>13.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2.07</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.26</td>\n",
       "      <td>14.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.93</td>\n",
       "      <td>2.25</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>14.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.80</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.23</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.74</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.22</td>\n",
       "      <td>14.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.22</td>\n",
       "      <td>14.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.94</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.25</td>\n",
       "      <td>14.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.23</td>\n",
       "      <td>15.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.02</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.27</td>\n",
       "      <td>15.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.94</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>15.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.74</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.24</td>\n",
       "      <td>15.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.24</td>\n",
       "      <td>15.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.24</td>\n",
       "      <td>16.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.24</td>\n",
       "      <td>16.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.75</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>16.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.04</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>16.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.74</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>16.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.95</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.27</td>\n",
       "      <td>16.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.82</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>16.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.25</td>\n",
       "      <td>16.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.25</td>\n",
       "      <td>16.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>16.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.09</td>\n",
       "      <td>2.52</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>16.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.26</td>\n",
       "      <td>17.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.26</td>\n",
       "      <td>17.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.83</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>17.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2.13</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.32</td>\n",
       "      <td>17.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.27</td>\n",
       "      <td>17.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>17.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2.03</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.31</td>\n",
       "      <td>17.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.26</td>\n",
       "      <td>17.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.82</td>\n",
       "      <td>2.23</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>18.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.76</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>18.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2.11</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.33</td>\n",
       "      <td>18.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2.13</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.33</td>\n",
       "      <td>18.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.85</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>18.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.89</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>18.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>19.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.81</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>19.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.85</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.30</td>\n",
       "      <td>19.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.78</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>19.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.94</td>\n",
       "      <td>2.46</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>21.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.72</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>21.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.80</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>21.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2.16</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.39</td>\n",
       "      <td>21.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.11</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>22.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.06</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>23.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.72</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>30.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1.74</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>32.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.74</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>34.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  target  residual  difference%\n",
       "128        1.97    1.96      0.00         0.14\n",
       "116        2.09    2.09     -0.00         0.21\n",
       "73         1.93    1.92      0.01         0.30\n",
       "126        2.01    2.01     -0.01         0.33\n",
       "47         2.07    2.07     -0.01         0.33\n",
       "37         1.69    1.70     -0.01         0.37\n",
       "112        2.14    2.13      0.01         0.44\n",
       "107        1.77    1.77     -0.01         0.45\n",
       "158        2.25    2.23      0.02         0.70\n",
       "74         2.01    2.03     -0.02         0.96\n",
       "40         1.75    1.76     -0.02         0.98\n",
       "28         1.93    1.91      0.02         1.00\n",
       "38         1.79    1.77      0.02         1.08\n",
       "152        2.10    2.07      0.02         1.14\n",
       "53         1.75    1.77     -0.02         1.26\n",
       "77         1.79    1.76      0.02         1.29\n",
       "122        2.31    2.28      0.03         1.46\n",
       "91         1.76    1.78     -0.03         1.55\n",
       "142        1.69    1.72     -0.03         1.78\n",
       "103        2.22    2.18      0.04         1.79\n",
       "49         1.73    1.70      0.03         1.89\n",
       "153        1.76    1.73      0.03         1.91\n",
       "150        2.18    2.22     -0.04         1.92\n",
       "117        1.73    1.70      0.04         2.07\n",
       "9          1.75    1.71      0.04         2.30\n",
       "154        2.32    2.27      0.06         2.44\n",
       "94         2.05    1.99      0.05         2.66\n",
       "2          1.75    1.70      0.05         3.14\n",
       "85         2.02    2.08     -0.07         3.25\n",
       "72         2.18    2.26     -0.08         3.32\n",
       "84         2.19    2.28     -0.08         3.71\n",
       "130        2.29    2.21      0.08         3.72\n",
       "80         2.08    2.17     -0.09         4.00\n",
       "88         2.26    2.17      0.09         4.03\n",
       "81         2.05    1.97      0.08         4.16\n",
       "105        2.08    1.99      0.08         4.17\n",
       "67         1.67    1.74     -0.07         4.18\n",
       "110        2.10    2.19     -0.09         4.29\n",
       "101        2.12    2.22     -0.10         4.36\n",
       "96         2.17    2.07      0.10         4.77\n",
       "134        2.04    1.94      0.10         5.08\n",
       "144        2.35    2.23      0.12         5.22\n",
       "138        2.18    2.06      0.11         5.43\n",
       "62         2.02    2.14     -0.12         5.51\n",
       "139        2.09    2.21     -0.12         5.54\n",
       "137        2.16    2.04      0.11         5.60\n",
       "39         2.15    2.28     -0.13         5.83\n",
       "109        2.22    2.09      0.12         5.84\n",
       "33         2.06    2.19     -0.13         5.91\n",
       "83         1.78    1.90     -0.13         6.60\n",
       "118        1.81    1.94     -0.13         6.60\n",
       "87         1.88    2.01     -0.14         6.81\n",
       "115        1.65    1.77     -0.12         6.99\n",
       "106        2.12    2.28     -0.16         7.04\n",
       "31         1.99    2.15     -0.16         7.30\n",
       "156        1.88    2.04     -0.16         7.88\n",
       "50         1.75    1.90     -0.15         7.97\n",
       "10         1.85    2.01     -0.16         8.00\n",
       "6          1.64    1.52      0.12         8.18\n",
       "147        1.76    1.92     -0.16         8.52\n",
       "78         1.65    1.52      0.13         8.55\n",
       "75         1.93    2.11     -0.18         8.63\n",
       "35         1.65    1.52      0.13         8.74\n",
       "86         1.64    1.51      0.13         8.89\n",
       "4          2.00    1.84      0.17         8.99\n",
       "132        1.78    1.96     -0.18         9.15\n",
       "26         2.03    2.24     -0.21         9.21\n",
       "54         1.65    1.51      0.14         9.28\n",
       "60         1.70    1.88     -0.18         9.54\n",
       "143        1.88    2.08     -0.20         9.61\n",
       "68         1.89    1.72      0.17         9.97\n",
       "127        1.85    2.05     -0.21        10.10\n",
       "3          2.06    2.30     -0.23        10.17\n",
       "114        2.14    1.94      0.20        10.34\n",
       "45         1.75    1.58      0.16        10.39\n",
       "34         1.65    1.49      0.16        10.57\n",
       "93         2.07    1.87      0.20        10.60\n",
       "119        2.11    1.91      0.20        10.64\n",
       "129        1.78    1.99     -0.21        10.74\n",
       "14         1.67    1.51      0.17        11.03\n",
       "21         1.67    1.51      0.17        11.11\n",
       "113        1.86    2.09     -0.24        11.29\n",
       "145        1.87    2.10     -0.24        11.31\n",
       "1          2.00    1.80      0.21        11.46\n",
       "125        1.68    1.51      0.17        11.53\n",
       "36         1.69    1.52      0.18        11.54\n",
       "141        1.87    2.11     -0.25        11.69\n",
       "66         1.68    1.51      0.18        11.70\n",
       "11         2.16    2.45     -0.29        11.71\n",
       "104        1.74    1.97     -0.23        11.88\n",
       "136        2.00    1.78      0.21        11.96\n",
       "111        1.77    2.01     -0.24        12.00\n",
       "133        2.06    2.35     -0.28        12.03\n",
       "157        2.12    1.89      0.23        12.21\n",
       "131        2.03    1.81      0.22        12.23\n",
       "13         1.76    1.57      0.19        12.29\n",
       "56         2.01    1.78      0.22        12.35\n",
       "59         2.15    2.45     -0.30        12.39\n",
       "18         1.69    1.51      0.19        12.50\n",
       "8          1.71    1.52      0.19        12.55\n",
       "55         1.95    1.73      0.22        12.74\n",
       "17         1.71    1.96     -0.25        12.77\n",
       "23         2.14    1.90      0.24        12.84\n",
       "41         1.72    1.52      0.20        13.05\n",
       "0          2.02    1.78      0.24        13.21\n",
       "92         2.04    1.81      0.24        13.21\n",
       "27         1.73    1.52      0.21        13.71\n",
       "12         2.04    1.80      0.25        13.73\n",
       "32         1.92    2.23     -0.31        13.78\n",
       "71         1.72    1.51      0.21        14.05\n",
       "120        2.07    1.81      0.26        14.35\n",
       "146        1.93    2.25     -0.32        14.36\n",
       "57         1.80    1.57      0.23        14.67\n",
       "89         1.74    1.52      0.22        14.75\n",
       "121        1.73    1.51      0.22        14.77\n",
       "20         1.94    1.69      0.25        14.92\n",
       "102        1.75    1.52      0.23        15.20\n",
       "5          2.02    1.75      0.27        15.22\n",
       "61         1.94    2.30     -0.36        15.56\n",
       "24         1.74    1.51      0.24        15.74\n",
       "52         1.76    1.52      0.24        15.85\n",
       "16         1.76    1.52      0.24        16.02\n",
       "19         1.72    1.48      0.24        16.07\n",
       "22         1.75    2.08     -0.34        16.12\n",
       "43         2.04    1.75      0.29        16.34\n",
       "7          1.74    1.49      0.24        16.37\n",
       "82         1.95    1.67      0.27        16.42\n",
       "149        1.82    2.18     -0.36        16.48\n",
       "151        1.76    1.51      0.25        16.56\n",
       "155        1.76    1.51      0.25        16.71\n",
       "58         1.88    2.27     -0.38        16.96\n",
       "42         2.09    2.52     -0.43        16.99\n",
       "29         1.78    1.52      0.26        17.14\n",
       "95         1.75    1.49      0.26        17.20\n",
       "48         1.83    2.22     -0.39        17.68\n",
       "63         2.13    1.81      0.32        17.74\n",
       "79         1.76    1.49      0.27        17.77\n",
       "108        1.71    2.08     -0.37        17.84\n",
       "51         2.03    1.72      0.31        17.86\n",
       "76         1.73    1.47      0.26        17.87\n",
       "97         1.82    2.23     -0.41        18.32\n",
       "100        1.76    2.15     -0.40        18.38\n",
       "135        2.11    1.78      0.33        18.47\n",
       "65         2.13    1.80      0.33        18.50\n",
       "15         1.85    2.27     -0.42        18.60\n",
       "44         1.89    2.33     -0.44        18.87\n",
       "69         1.78    1.49      0.28        19.01\n",
       "124        1.81    2.24     -0.43        19.40\n",
       "98         1.85    1.54      0.30        19.48\n",
       "70         1.78    2.22     -0.44        19.97\n",
       "90         1.94    2.46     -0.52        21.18\n",
       "148        1.72    2.19     -0.47        21.50\n",
       "140        1.80    2.30     -0.50        21.63\n",
       "99         2.16    1.77      0.39        21.72\n",
       "25         2.11    2.72     -0.61        22.39\n",
       "30         2.06    2.70     -0.64        23.65\n",
       "64         1.72    2.48     -0.76        30.51\n",
       "123        1.74    2.59     -0.85        32.73\n",
       "46         1.74    2.65     -0.91        34.34"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "perf.sort_values(by = ['difference%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 (testing) = 0.32303021426665435\n"
     ]
    }
   ],
   "source": [
    "r2_test = metrics.r2_score(y_test, y_hat_test)\n",
    "print('R2 (testing) = {}'.format(r2_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
