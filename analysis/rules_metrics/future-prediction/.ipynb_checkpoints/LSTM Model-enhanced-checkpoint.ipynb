{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict future latency using LTSM\n",
    "Haytham Mohamed - INFS890 - Spring 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/infs890/bin/python\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import sys\n",
    "\n",
    "print(sys.executable)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/Users/hmohamed/github/data-research-spring2020/raw-data-linode-run3-rules/'\n",
    "\n",
    "#input_file = 'raw_timeseries_data.csv'\n",
    "input_file = 'no_outliers_dataset.csv'\n",
    "\n",
    "output_dir = input_dir + 'models/'\n",
    "model_checkpoint_file='lstm_model_checkpoint.keras'\n",
    "\n",
    "log_dir = output_dir + 'models/model_logs/'\n",
    "\n",
    "\n",
    "target = 'svc_ltcy_200'\n",
    "\n",
    "RUN_LSTM = True\n",
    "\n",
    "SCALE_TARGET = False\n",
    "SCALE_FEATURES = True\n",
    "\n",
    "# training window steps\n",
    "# train with n number of minutes worth of sequence. \n",
    "# note: each observation is scrapped every 15 seconds\n",
    "# every 4 oberservations are for a minute time window\n",
    "# 1 minute = 4 observations\n",
    "# 1 hour = 1 * 60 * 4 observations  .. and so on\n",
    "training_window = 1 * 60 * 4\n",
    "\n",
    "# prediction steps\n",
    "# again 1 observation is scrapped every 15 seconds\n",
    "# predicting a 5 minutes ahead, is equivalent to skipping 5 * 4 observations\n",
    "prediction_minutes =  5\n",
    "\n",
    "shift_steps = prediction_minutes * 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>svc_resp_size</th>\n",
       "      <th>svc_cpu_use</th>\n",
       "      <th>svc_req_rate</th>\n",
       "      <th>system_net_use</th>\n",
       "      <th>svc_pods</th>\n",
       "      <th>svc_net_use</th>\n",
       "      <th>system_cpu_use</th>\n",
       "      <th>svc_ltcy_200</th>\n",
       "      <th>svc_disk_use</th>\n",
       "      <th>svc_req_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-27 22:49:50</td>\n",
       "      <td>20.460420</td>\n",
       "      <td>2.166677</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.579341</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.571974</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.96</td>\n",
       "      <td>5.032348</td>\n",
       "      <td>1.921885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-27 22:50:35</td>\n",
       "      <td>40.236221</td>\n",
       "      <td>-6.739642</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3.694395</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.534885</td>\n",
       "      <td>3.18</td>\n",
       "      <td>3.93</td>\n",
       "      <td>2.163715</td>\n",
       "      <td>5.850586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-27 22:50:50</td>\n",
       "      <td>38.784131</td>\n",
       "      <td>1.954651</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.636447</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.661786</td>\n",
       "      <td>3.39</td>\n",
       "      <td>4.00</td>\n",
       "      <td>26.683041</td>\n",
       "      <td>5.593096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  svc_resp_size  svc_cpu_use  svc_req_rate  \\\n",
       "0  2020-02-27 22:49:50      20.460420     2.166677          1.29   \n",
       "1  2020-02-27 22:50:35      40.236221    -6.739642          3.56   \n",
       "2  2020-02-27 22:50:50      38.784131     1.954651          3.38   \n",
       "\n",
       "   system_net_use  svc_pods  svc_net_use  system_cpu_use  svc_ltcy_200  \\\n",
       "0        2.579341       7.0    -0.571974            2.77          0.96   \n",
       "1        3.694395       7.0     2.534885            3.18          3.93   \n",
       "2        3.636447       7.0    -0.661786            3.39          4.00   \n",
       "\n",
       "   svc_disk_use  svc_req_size  \n",
       "0      5.032348      1.921885  \n",
       "1      2.163715      5.850586  \n",
       "2     26.683041      5.593096  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = pd.read_csv(input_dir + input_file)\n",
    "metrics_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2318, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'service_ltcy_200' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5e01ad28593a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# move target column to the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmetrics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'service_ltcy_200' is not in list"
     ]
    }
   ],
   "source": [
    "# move target column to the end \n",
    "cols = metrics_df.columns.tolist()\n",
    "cols.insert(len(cols)-1, cols.pop(cols.index(target)))\n",
    "metrics_df = metrics_df.reindex(columns= cols)\n",
    "\n",
    "# index by date and sort\n",
    "metrics_df.date = pd.to_datetime(metrics_df.date)\n",
    "metrics_df.set_index('date', inplace=True)\n",
    "metrics_df.sort_index()\n",
    "\n",
    "metrics_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = metrics_df.values\n",
    "columns = metrics_df.columns\n",
    "\n",
    "# specify columns to plot\n",
    "number_of_features = len(columns) - 1\n",
    "\n",
    "i = 1\n",
    "## plot each column\n",
    "plt.figure(figsize=(20, 40), dpi=80, facecolor='w', edgecolor='k')\n",
    "for group in range(number_of_features):\n",
    "    plt.subplot(number_of_features, 1, i)\n",
    "    plt.plot(values[:, group])\n",
    "    plt.title(columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shift target Data for Prediction\n",
    "\n",
    "Assuming a 1 minute ahead prediction.\n",
    "\n",
    "The following is the number of time-steps that we will shift the target-data. Our data-set is resampled to have an observation every 1 second, so there are 60 observations for every minute.\n",
    "\n",
    "If we want to predict the latency 1 minute into the future, we shift the data 4 time-steps. If we want to predict the weather 2 minutes into the future, we shift the data 2 * 60 time-steps, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Target data series\n",
    "df_targets = metrics_df[target].shift(-shift_steps)\n",
    "df_targets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double checking the shifted data\n",
    "metrics_df[target].head(shift_steps + 5)  # before shifting. Note the 5th record shifted first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets.tail()  # note last recrods are shifted up and no more values on them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data as Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = metrics_df[0:-shift_steps]\n",
    "x_data = x_data.values\n",
    "print(\"Shape x_data: \", x_data.shape)\n",
    "print(\"type x_data: \", type(x_data))\n",
    "\n",
    "y_data = df_targets.values[:-shift_steps]\n",
    "y_data = y_data.reshape(-1,1)\n",
    "print(\"Shape y_data: \", y_data.shape)\n",
    "print(\"type y_data: \", type(y_data))\n",
    "\n",
    "num_data = len(x_data)\n",
    "print(\"There are {} sample data\".format(num_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "if SCALE_FEATURES:\n",
    "    x_scaler = MinMaxScaler()  # StandardScaler()\n",
    "    x_data = x_scaler.fit_transform(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCALE_TARGET:\n",
    "    y_scaler =  MinMaxScaler() # StandardScaler()\n",
    "    y_data = y_scaler.fit_transform(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack\n",
    "\n",
    "dataset = hstack((x_data, y_data))\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dataset, y_dataset = split_sequences(dataset, training_window)\n",
    "\n",
    "print(\"x_dataset shape {} and y_dataset shape {}\".format(x_dataset.shape,y_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_split = 0.1\n",
    "test_split = 0.1\n",
    "train_split = 1 - (test_split + validate_split)\n",
    "\n",
    "num_test = int(test_split * num_data)\n",
    "num_validate = int(validate_split * num_data)\n",
    "num_train = num_data - (num_test + num_validate)\n",
    "print('Data splitted to training {} , validation {} and testing {}'.format(num_train, num_validate, num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_dataset[:num_train]\n",
    "x_validate = x_dataset[num_train:num_train+num_validate]\n",
    "x_test = x_dataset[num_train+num_validate:]\n",
    "\n",
    "print(\"x_train {} , x_validate {} , x_test {}\".format(x_train.shape, x_validate.shape, x_test.shape))\n",
    "\n",
    "len(x_train) + len(x_validate) + len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_dataset[:num_train]\n",
    "y_validate = y_dataset[num_train:num_train+num_validate]\n",
    "y_test = y_dataset[num_train+num_validate:]\n",
    "\n",
    "print(\"y_train {} , y_validate {} , y_test {}\".format(y_train.shape, y_validate.shape, y_test.shape))\n",
    "\n",
    "len(y_train) + len(y_validate) + len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y_test shape {}'.format(y_test.shape))\n",
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_x_signals = x_dataset.shape[2]\n",
    "num_x_signals\n",
    "\n",
    "num_y_signals = y_data.shape[1]\n",
    "num_y_signals\n",
    "\n",
    "print('Experiment has {} number of features and {} number of target(s)'.format(num_x_signals,num_y_signals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate LSTM Forecast Model\n",
    "\n",
    "We need to model a forecasting model that can predict set of metrics given past observed metrics. I will fit an LSTM model to the collected metrics data. \n",
    "\n",
    "Instead of training the Recurrent Neural Network on the complete sequences of all observations, we will use the following function to create a batch of shorter sub-sequences picked at random from the training-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, sequence_length=training_window):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
    "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
    "\n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        #y_shape = (batch_size, sequence_length, num_y_signals)\n",
    "        #y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
    "        y_batch = []\n",
    "\n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = np.random.randint(num_train - sequence_length)\n",
    "            \n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_train[idx]\n",
    "            y_batch.append(y_train[idx])\n",
    "        \n",
    "        yield (x_batch, np.array(y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a large batch-size so as to keep the GPU near 100% work-load. You may have to adjust this number depending on your GPU, its RAM and your choice of sequence_length below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the batch-generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = batch_generator(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then test the batch-generator to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a random batch of 246, each batch has 240 observations, and each observation has 13 input-signals and 1 output-signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot one of the 20 input-signals as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 0   # First sequence in the batch.\n",
    "signal = 1  # First signal from the 20 input-signals.\n",
    "seq = x_batch[batch, :, signal]\n",
    "plt.plot(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot one of the output-signals that we want the model to learn how to predict given all those 20 input signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = y_batch\n",
    "plt.plot(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the validation-data we will instead run through the entire sequence from the test-set and measure the prediction accuracy on that entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = (x_test, y_test)\n",
    "validate_data = (x_validate, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTSM Model\n",
    "\n",
    "\n",
    "We will use Mean Squared Error (MSE) as the loss-function that will be minimized. This measures how closely the model's output matches the true output signals.\n",
    "\n",
    "However, at the beginning of a sequence, the model has only seen input-signals for a few time-steps, so its generated output may be very inaccurate. Using the loss-value for the early time-steps may cause the model to distort its later output. We therefore give the model a \"warmup-period\" of 50 time-steps where we don't use its accuracy in the loss-function, in hope of improving the accuracy for later time-steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, LSTM\n",
    "from tensorflow.python.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse_warmup(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error between y_true and y_pred,\n",
    "    but ignore the beginning \"warmup\" part of the sequences.\n",
    "    \n",
    "    y_true is the desired output.\n",
    "    y_pred is the model's output.\n",
    "    \"\"\"\n",
    "\n",
    "    # The shape of both input tensors are:\n",
    "    # [batch_size, sequence_length, num_y_signals].\n",
    "\n",
    "    # Ignore the \"warmup\" parts of the sequences\n",
    "    # by taking slices of the tensors.\n",
    "    y_true_slice = y_true[:warmup_steps]\n",
    "    y_pred_slice = y_pred[:warmup_steps]\n",
    "\n",
    "    # These sliced tensors both have this shape:\n",
    "    # [batch_size, sequence_length - warmup_steps, num_y_signals]\n",
    "\n",
    "    # Calculate the MSE loss for each value in these tensors.\n",
    "    # This outputs a 3-rank tensor of the same shape.\n",
    "    loss = tf.losses.mean_squared_error(labels=y_true_slice,\n",
    "                                        predictions=y_pred_slice)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire tensor, we reduce it to a\n",
    "    # single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define and fit an LSTM model.\n",
    "\n",
    "I will define the LSTM with 50 neurons in the first hidden layer and 1 neuron in the output layer for predicting pollution. The input shape will be 1 time step with 14 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_nodes_number_1 = 30\n",
    "hidden_nodes_number_2 = 15\n",
    "hidden_nodes_number_3 = 8\n",
    "\n",
    "initialize=False\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "if initialize:\n",
    "    from tensorflow.python.keras.initializers import RandomUniform\n",
    "    init = RandomUniform(minval=-0.05, maxval=0.05)\n",
    "    model.add(LSTM(hidden_nodes_number_1, kernel_initializer=init, input_shape=(None, num_x_signals,)))\n",
    "    #model.add(LSTM(hidden_nodes_number_1, kernel_initializer=init, return_sequences=True, activation='relu'))\n",
    "    model.add(Dense(hidden_nodes_number_2, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(hidden_nodes_number_3, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='linear'))\n",
    "else:\n",
    "    model.add(LSTM(hidden_nodes_number_1, kernel_initializer='normal', input_shape=(None, num_x_signals,)))\n",
    "    #model.add(LSTM(hidden_nodes_number_1, kernel_initializer=init, return_sequences=True, activation='relu'))\n",
    "    model.add(Dense(hidden_nodes_number_2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(hidden_nodes_number_3, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "\n",
    "#model.compile(loss='mae', optimizer='adam')\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "#optimizer = RMSprop(lr=1e-3)\n",
    "optimizer = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model.compile(loss=loss_mse_warmup, optimizer=optimizer)\n",
    "#model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Callback Functions\n",
    "\n",
    "During training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras.\n",
    "\n",
    "This is the callback for writing checkpoints during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = output_dir + model_checkpoint_file\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the callback for stopping the optimization when performance worsens on the validation-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the callback for writing the TensorBoard log during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(log_dir=log_dir,\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback reduces the learning-rate for the optimizer if the validation-loss has not improved since the last epoch (as indicated by patience=0). The learning-rate will be reduced by multiplying it with the given factor. We set a start learning-rate of 1e-3 above, so multiplying it by 0.1 gives a learning-rate of 1e-4. We don't want the learning-rate to go any lower than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard,\n",
    "             callback_reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 20\n",
    "batch_size = 1000\n",
    "\n",
    "steps_per_epoch = int(num_train / batch_size)   # = total number of training data points divided by the batch size\n",
    "\n",
    "if RUN_LSTM:\n",
    "    history = model.fit_generator(generator=generator,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validate_data,\n",
    "                    callbacks=callbacks, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "if RUN_LSTM:\n",
    "    plt.figure(figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(x=x_test, y=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"loss (test-set):\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have several metrics you can use this instead.\n",
    "if False:\n",
    "    for res, metric in zip(result, model.metrics_names):\n",
    "        print(\"{0}: {1:.3e}\".format(metric, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_comparison(start_idx, length=100, target_scaled=True, data_set='train'):\n",
    "    \"\"\"\n",
    "    Obtain predicted and true output-signals.\n",
    "    \n",
    "    :param start_idx: Start-index for the time-series.\n",
    "    :param length: Sequence-length to process and plot.\n",
    "    :param train: Boolean whether to use training- or test-set.\n",
    "    \"\"\"\n",
    "    \n",
    "    if data_set == 'train':\n",
    "        # Use training-data.\n",
    "        x = x_train\n",
    "        y_true = y_train\n",
    "    elif data_set == 'validate':\n",
    "        x = x_validate\n",
    "        y_true = y_validate\n",
    "    else:\n",
    "        # Use test-data.\n",
    "        x = x_test\n",
    "        y_true = y_test\n",
    "    \n",
    "    # End-index for the sequences.\n",
    "    end_idx = start_idx + length\n",
    "    \n",
    "    # Select the sequences from the given start-index and\n",
    "    # of the given length.\n",
    "    x = x[start_idx:end_idx]\n",
    "    y_true = y_true[start_idx:end_idx]\n",
    "    \n",
    "    # Input-signals for the model.\n",
    "    #x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    # Use the model to predict the output-signals.\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    if target_scaled:\n",
    "        # The output of the model is scaled.\n",
    "        # Do an inverse map to get it back to the scale\n",
    "        # of the original data-set.\n",
    "        y_pred = y_scaler.inverse_transform(y_pred)\n",
    "    \n",
    "    print(\"shape y_pred: %\", y_pred.shape)\n",
    "    print(\"shape y_true: %\", y_true.shape)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "#This helper-function plots the predicted and true output-signals.\n",
    "def plot_comparison(signal_true, signal_pred):\n",
    "    \"\"\"\n",
    "    Plot the predicted and true output-signals.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make the plotting-canvas bigger.\n",
    "    plt.figure(figsize=(15,5))\n",
    "        \n",
    "    # Plot and compare the two signals.\n",
    "    plt.plot(signal_true, label='true')\n",
    "    plt.plot(signal_pred, label='pred')\n",
    "        \n",
    "    # Plot grey box for warmup-period.\n",
    "    p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.15)\n",
    "        \n",
    "    # Plot labels etc.\n",
    "    plt.ylabel('ltcy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with an example from the training-data. This is data that the model has seen during training so it should perform reasonably well on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_signal_true, train_signal_pred = y_comparison(start_idx=10, length=1000\n",
    "                                                    , target_scaled=SCALE_TARGET, data_set='train')\n",
    "plot_comparison(train_signal_true, train_signal_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_signal_pred.reshape(-1,1000)\n",
    "\n",
    "print('shape y test {}'.format(train_signal_true.shape))\n",
    "print('shape y predicted {}'.format(train_signal_pred.shape))\n",
    "\n",
    "#print (\"Actual train values: \\n {}\".format(train_signal_true[:20]))\n",
    "#print (\"Predicted train values: \\n {}\".format(train_signal_pred[:20]))\n",
    "\n",
    "print(\"\")\n",
    "# calculate RMSE\n",
    "rmse = math.sqrt(mean_squared_error(train_signal_true, train_signal_pred))\n",
    "print('Train RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_signal_true, train_signal_pred, alpha=0.2)\n",
    "plt.xlabel('Latency', size=18)\n",
    "plt.ylabel('Predicted Latency', size=18)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signal_true, test_signal_pred = y_comparison(start_idx=10, length=1000\n",
    "                                                    , target_scaled=SCALE_TARGET, data_set='test')\n",
    "plot_comparison(test_signal_true, test_signal_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_signal_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f8a33f779c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape y test {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_signal_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape y predicted {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_signal_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print (\"Actual test values: \\n {}\".format(test_signal_true[:20]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print (\"Predicted test values: \\n {}\".format(test_signal_pred[:20]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_signal_true' is not defined"
     ]
    }
   ],
   "source": [
    "print('shape y test {}'.format(test_signal_true.shape))\n",
    "print('shape y predicted {}'.format(test_signal_pred.shape))\n",
    "\n",
    "#print (\"Actual test values: \\n {}\".format(test_signal_true[:20]))\n",
    "#print (\"Predicted test values: \\n {}\".format(test_signal_pred[:20]))\n",
    "\n",
    "print(\"\")\n",
    "# calculate RMSE\n",
    "rmse = math.sqrt(mean_squared_error(test_signal_true, test_signal_pred))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_signal_true, test_signal_pred, alpha=0.2)\n",
    "plt.xlabel('Latency', size=18)\n",
    "plt.ylabel('Predicted Latency', size=18)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['f'] = df['d'].rolling(2).sum().shift(-1)\n",
    "\n",
    "print(test_signal_true.shape)\n",
    "print(test_signal_pred.shape)\n",
    "\n",
    "true_df = round(pd.DataFrame(test_signal_true, columns=['actual']),2)\n",
    "pred_df = round(pd.DataFrame(test_signal_pred, columns=['predicted']),2)\n",
    "\n",
    "\n",
    "compare_df = pd.concat([true_df,pred_df], axis=1, sort=False)\n",
    "\n",
    "# testing mean_absolute_percentage_error\n",
    "compare_df['residual'] = round(compare_df['actual'] - compare_df['predicted'], 2)\n",
    "compare_df['difference%'] = round(np.absolute(compare_df['residual']* 100 / compare_df['actual']),2)\n",
    "\n",
    "compare_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "compare_df.sort_values(by = ['difference%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 is a statistic that will give some information about the goodness of fit of a model. In regression, the R2 coefficient of determination is a statistical measure of how well the regression predictions approximate the real data points. An R2 of 1 indicates that the regression predictions perfectly fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-Squared \n",
    "# = 1 -  ( sum of squared residual  / sum of squared variance )\n",
    "# sum of variance squared = sum (( y - y-mean ) ** 2)\n",
    "# sum of residual squared = sum (( y-hat - y-mean ) ** 2)\n",
    "# where y is the observed target, y-mean is the mean of observed target, y-hat is a predicted value\n",
    "\n",
    "y_mean = np.mean(test_signal_true)\n",
    "sum_squared_residual = np.sum(np.power(compare_df['residual'] , 2))\n",
    "sum_squared_variance = np.sum(np.power(test_signal_true - y_mean , 2))\n",
    "\n",
    "R_squared = 1 - ( sum_squared_residual / sum_squared_variance )\n",
    "\n",
    "R_squared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
