{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.callbacks import EarlyStopping , ReduceLROnPlateau , ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "sns.set(color_codes=True)\n",
    "#sns.set_color_codes()\n",
    "\n",
    "pd.options.display.max_rows = 15\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network structure. \n",
    "# not to include the input layer\n",
    "net_layers = (8, 5)  # (11,9,8,4)\n",
    "\n",
    "epochs=1000\n",
    "batch_size=100\n",
    "\n",
    "learning_rate = 1e-3\n",
    "decay = learning_rate / epochs\n",
    "\n",
    "patience=80\n",
    "\n",
    "test_split = 0.1\n",
    "validation_split = 0.2\n",
    "\n",
    "select_features=True\n",
    "\n",
    "# trials\n",
    "# 11 R2-> -0.12\n",
    "# 11,16,8,4 R2-> 0.37\n",
    "# 11,11,6 R2-> -0.2\n",
    "# 11,8,6 R2->   0.316\n",
    "# 11,11,8,4 R2->  0.37\n",
    "# 11, 6 R2-> 0.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- read data file\n",
    "# 1- read processed file\n",
    "home_dir = '/Users/hmohamed/github/data-research-spring2020/sock-shop'\n",
    "file_dir = home_dir + '/processed-data/'\n",
    "models_dir = home_dir + \"/models/\"\n",
    "\n",
    "data_file = 'orders_flow_data.csv'   # raw data as is\n",
    "#data_file = 'order_flow_cleanup_data.csv'\n",
    "#data_file = 'order_flow_normalized_data.csv'\n",
    "#data_file = 'order_flow_standardized_data.csv'\n",
    "#data_file = 'order_flow_factored_data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(file_dir, data_file, timeseries=True):\n",
    "    df = pd.read_csv(file_dir + data_file)\n",
    "    if timeseries:\n",
    "        df = to_time_series(df)\n",
    "    return df\n",
    "\n",
    "def to_time_series(df, index_col_name='date'):\n",
    "    df[index_col_name] = pd.to_datetime(df[index_col_name])\n",
    "    df.set_index(index_col_name, inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def merge(df, series):\n",
    "    return pd.merge_asof(df, series, left_index=True, right_index=True, tolerance=pd.Timedelta('1 second')).bfill()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>front-end_cpu_use</th>\n",
       "      <th>orders_cpu_use</th>\n",
       "      <th>orders-db_cpu_use</th>\n",
       "      <th>user_cpu_use</th>\n",
       "      <th>user-db_cpu_use</th>\n",
       "      <th>shipping_cpu_use</th>\n",
       "      <th>payment_cpu_use</th>\n",
       "      <th>carts_cpu_use</th>\n",
       "      <th>carts-db_cpu_use</th>\n",
       "      <th>front-end_pods</th>\n",
       "      <th>orders_pods</th>\n",
       "      <th>user_pods</th>\n",
       "      <th>shipping_pods</th>\n",
       "      <th>payment_pods</th>\n",
       "      <th>carts_pods</th>\n",
       "      <th>nodes_cpu_use</th>\n",
       "      <th>orders_req</th>\n",
       "      <th>orders_ltcy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   front-end_cpu_use  orders_cpu_use  orders-db_cpu_use  user_cpu_use  \\\n",
       "0               0.03            0.01               0.01          0.01   \n",
       "1               0.06            0.02               0.02          0.02   \n",
       "2               0.07            0.03               0.02          0.03   \n",
       "3               0.07            0.03               0.02          0.02   \n",
       "4               0.07            0.02               0.01          0.02   \n",
       "\n",
       "   user-db_cpu_use  shipping_cpu_use  payment_cpu_use  carts_cpu_use  \\\n",
       "0             0.01              0.01             0.01           0.03   \n",
       "1             0.02              0.01             0.01           0.18   \n",
       "2             0.02              0.01             0.01           0.44   \n",
       "3             0.02              0.01             0.01           0.23   \n",
       "4             0.02              0.01             0.01           0.24   \n",
       "\n",
       "   carts-db_cpu_use  front-end_pods  orders_pods  user_pods  shipping_pods  \\\n",
       "0              0.07            1.00         1.00       1.00           1.00   \n",
       "1              0.18            1.00         1.00       1.00           1.00   \n",
       "2              0.20            1.00         1.00       1.00           1.00   \n",
       "3              0.17            1.00         1.00       1.00           1.00   \n",
       "4              0.14            1.00         1.00       1.00           1.00   \n",
       "\n",
       "   payment_pods  carts_pods  nodes_cpu_use  orders_req  orders_ltcy  \n",
       "0          1.00        1.00           2.50        0.15         0.24  \n",
       "1          1.00        1.00           1.89        0.15         0.24  \n",
       "2          1.00        1.00           1.35        0.15         0.24  \n",
       "3          1.00        1.00           1.26        0.13         0.24  \n",
       "4          1.00        1.00           1.28        0.36         0.24  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_df(file_dir, data_file, True)\n",
    "data = data.reset_index(drop=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6766, 18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6766, 1, 18)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to 3D\n",
    "data = data.values.reshape(data.shape[0], 1, data.shape[1])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 17 features\n"
     ]
    }
   ],
   "source": [
    "targets = data['orders_ltcy']\n",
    "inputs = data.drop(['orders_ltcy'], axis=1)\n",
    "\n",
    "n_features = inputs.values.shape[1]\n",
    "print(\"there are {} features\".format(n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = len(feature_list)\n",
    "n_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (1818,) , y_test (203,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(inputs, targets, test_size=test_split, shuffle=True, random_state=365)\n",
    "\n",
    "print(\"y_train {} , y_test {}\".format(y_train.shape, y_test.shape))\n",
    "\n",
    "# for better convergence and result scale target to values between 0 - 1\n",
    "y_train_max = y_train.max()\n",
    "y_test_max = y_test.max()\n",
    "\n",
    "y_train = y_train / y_train_max\n",
    "y_test = y_test / y_test_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)  # fit on training data only\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(nodes = net_layers):   # this does not work with KerasRegressor. interesting\n",
    "    # create model\n",
    "    model = Sequential()  \n",
    "    \n",
    "    if isinstance(nodes, int) or not nodes:\n",
    "        model.add(Dense(1, input_dim=n_inputs, kernel_initializer='normal'))\n",
    "    \n",
    "    else:\n",
    "        model.add(Dense(nodes[1], input_dim=n_inputs, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "        layer = 0\n",
    "        while layer < len(nodes):\n",
    "            model.add(Dense(nodes[layer], kernel_initializer='normal', activation='relu'))\n",
    "            layer = layer + 1\n",
    "        \n",
    "        #model.add(Dense(1, kernel_initializer='normal', activation='linear')) \n",
    "        model.add(Dense(1, kernel_initializer='normal', activation='linear'))  \n",
    "    \n",
    "    adam = Adam(lr=learning_rate, decay=decay)\n",
    "    \n",
    "    # or loss= 'mean_absolute_percentage_error'\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mse'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1454 samples, validate on 364 samples\n",
      "Epoch 1/600\n",
      "1454/1454 [==============================] - 5s 3ms/step - loss: 0.4664 - mean_squared_error: 0.4664 - val_loss: 0.4525 - val_mean_squared_error: 0.4525\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45247, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 2/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.4461 - mean_squared_error: 0.4461 - val_loss: 0.4319 - val_mean_squared_error: 0.4319\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45247 to 0.43194, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 3/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.4250 - mean_squared_error: 0.4250 - val_loss: 0.4107 - val_mean_squared_error: 0.4107\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43194 to 0.41070, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 4/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.4029 - mean_squared_error: 0.4029 - val_loss: 0.3874 - val_mean_squared_error: 0.3874\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.41070 to 0.38743, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 5/600\n",
      "1454/1454 [==============================] - 0s 35us/step - loss: 0.3766 - mean_squared_error: 0.3766 - val_loss: 0.3568 - val_mean_squared_error: 0.3568\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.38743 to 0.35682, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 6/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.3368 - mean_squared_error: 0.3368 - val_loss: 0.3036 - val_mean_squared_error: 0.3036\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35682 to 0.30361, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 7/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.2607 - mean_squared_error: 0.2607 - val_loss: 0.2013 - val_mean_squared_error: 0.2013\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30361 to 0.20133, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 8/600\n",
      "1454/1454 [==============================] - 0s 34us/step - loss: 0.1385 - mean_squared_error: 0.1385 - val_loss: 0.0961 - val_mean_squared_error: 0.0961\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.20133 to 0.09613, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 9/600\n",
      "1454/1454 [==============================] - 0s 34us/step - loss: 0.0819 - mean_squared_error: 0.0819 - val_loss: 0.0856 - val_mean_squared_error: 0.0856\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09613 to 0.08556, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 10/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0692 - mean_squared_error: 0.0692 - val_loss: 0.0736 - val_mean_squared_error: 0.0736\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08556 to 0.07359, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 11/600\n",
      "1454/1454 [==============================] - 0s 35us/step - loss: 0.0633 - mean_squared_error: 0.0633 - val_loss: 0.0675 - val_mean_squared_error: 0.0675\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.07359 to 0.06751, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 12/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0593 - mean_squared_error: 0.0593 - val_loss: 0.0632 - val_mean_squared_error: 0.0632\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.06751 to 0.06316, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 13/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0563 - mean_squared_error: 0.0563 - val_loss: 0.0598 - val_mean_squared_error: 0.0598\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.06316 to 0.05981, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 14/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0541 - mean_squared_error: 0.0541 - val_loss: 0.0573 - val_mean_squared_error: 0.0573\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.05981 to 0.05726, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 15/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0525 - mean_squared_error: 0.0525 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.05726 to 0.05529, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 16/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0513 - mean_squared_error: 0.0513 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.05529 to 0.05381, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 17/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0503 - mean_squared_error: 0.0503 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.05381 to 0.05272, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 18/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0497 - mean_squared_error: 0.0497 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.05272 to 0.05191, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 19/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05191 to 0.05132, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 20/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.05132 to 0.05087, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 21/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0484 - mean_squared_error: 0.0484 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.05087 to 0.05051, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 22/600\n",
      "1454/1454 [==============================] - 0s 35us/step - loss: 0.0481 - mean_squared_error: 0.0481 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.05051 to 0.05022, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 23/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.05022 to 0.04996, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 24/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0475 - mean_squared_error: 0.0475 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.04996 to 0.04973, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 25/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0473 - mean_squared_error: 0.0473 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.04973 to 0.04951, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 26/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0471 - mean_squared_error: 0.0471 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.04951 to 0.04929, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 27/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0468 - mean_squared_error: 0.0468 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.04929 to 0.04910, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 28/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0466 - mean_squared_error: 0.0466 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.04910 to 0.04891, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 29/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0464 - mean_squared_error: 0.0464 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.04891 to 0.04873, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 30/600\n",
      "1454/1454 [==============================] - 0s 35us/step - loss: 0.0462 - mean_squared_error: 0.0462 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.04873 to 0.04852, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 31/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04852 to 0.04831, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 32/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0457 - mean_squared_error: 0.0457 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04831 to 0.04814, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 33/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04814 to 0.04798, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 34/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04798 to 0.04782, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 35/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04782 to 0.04766, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 36/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0449 - mean_squared_error: 0.0449 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04766 to 0.04751, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 37/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04751 to 0.04736, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 38/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.04736 to 0.04720, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 39/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.04720 to 0.04706, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 40/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.04706 to 0.04692, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 41/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.04692 to 0.04677, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 42/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.04677 to 0.04663, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 43/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0437 - mean_squared_error: 0.0437 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.04663 to 0.04650, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 44/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.04650 to 0.04638, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 45/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0434 - mean_squared_error: 0.0434 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.04638 to 0.04626, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 46/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0433 - mean_squared_error: 0.0433 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.04626 to 0.04615, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 47/600\n",
      "1454/1454 [==============================] - 0s 34us/step - loss: 0.0432 - mean_squared_error: 0.0432 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.04615 to 0.04604, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 48/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0431 - mean_squared_error: 0.0431 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.04604 to 0.04595, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 49/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0429 - mean_squared_error: 0.0429 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.04595 to 0.04595, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 50/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0429 - mean_squared_error: 0.0429 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.04595\n",
      "Epoch 51/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0429 - mean_squared_error: 0.0429 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.04595\n",
      "Epoch 52/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0429 - mean_squared_error: 0.0429 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.04595 to 0.04594, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 53/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.04594 to 0.04593, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 54/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.04593 to 0.04592, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 55/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.04592 to 0.04591, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 56/600\n",
      "1454/1454 [==============================] - 0s 35us/step - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.04591 to 0.04590, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 57/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.04590 to 0.04589, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 58/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.04589 to 0.04588, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 59/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.04588 to 0.04587, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 60/600\n",
      "1454/1454 [==============================] - 0s 34us/step - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.04587 to 0.04586, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 61/600\n",
      "1454/1454 [==============================] - 0s 35us/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.04586 to 0.04584, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 62/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.04584 to 0.04583, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 63/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.04583 to 0.04582, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 64/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.04582 to 0.04581, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 65/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.04581 to 0.04580, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 66/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.04580 to 0.04579, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 67/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.04579 to 0.04578, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 68/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.04578 to 0.04577, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 69/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.04577 to 0.04576, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 70/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.04576 to 0.04574, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 71/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.04574 to 0.04573, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 72/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.04573 to 0.04572, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 73/600\n",
      "1454/1454 [==============================] - 0s 34us/step - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.04572 to 0.04571, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 74/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.04571 to 0.04570, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 75/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.04570 to 0.04569, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 76/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.04569 to 0.04568, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 77/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454/1454 [==============================] - 0s 34us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.04568 to 0.04567, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 78/600\n",
      "1454/1454 [==============================] - 0s 33us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.04567 to 0.04566, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 79/600\n",
      "1454/1454 [==============================] - 0s 35us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.04566 to 0.04565, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 80/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.04565 to 0.04564, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 81/600\n",
      "1454/1454 [==============================] - 0s 34us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.04564 to 0.04563, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 82/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.04563 to 0.04562, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 83/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.04562 to 0.04561, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 84/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.04561 to 0.04560, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 85/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.04560 to 0.04559, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 86/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.04559 to 0.04558, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 87/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.04558 to 0.04558, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 88/600\n",
      "1454/1454 [==============================] - 0s 66us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.04558 to 0.04557, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 89/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.04557 to 0.04556, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 90/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.04556 to 0.04555, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 91/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.04555 to 0.04554, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 92/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.04554 to 0.04553, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 93/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.04553 to 0.04552, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 94/600\n",
      "1454/1454 [==============================] - 0s 59us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.04552 to 0.04552, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 95/600\n",
      "1454/1454 [==============================] - 0s 60us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.04552 to 0.04551, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 96/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.04551 to 0.04550, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 97/600\n",
      "1454/1454 [==============================] - 0s 66us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.04550 to 0.04549, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 98/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.04549 to 0.04548, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 99/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.04548 to 0.04547, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 100/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.04547 to 0.04546, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 101/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.04546 to 0.04545, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 102/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00102: val_loss improved from 0.04545 to 0.04545, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 103/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.04545 to 0.04544, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 104/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.04544 to 0.04543, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 105/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.04543 to 0.04542, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 106/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.04542 to 0.04541, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 107/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.04541 to 0.04540, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 108/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.04540 to 0.04540, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 109/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.04540 to 0.04539, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 110/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.04539 to 0.04538, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 111/600\n",
      "1454/1454 [==============================] - 0s 59us/step - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.04538 to 0.04537, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 112/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.04537 to 0.04536, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 113/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.04536 to 0.04535, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 114/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.04535 to 0.04535, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 115/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.04535 to 0.04534, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 116/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.04534 to 0.04533, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 117/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.04533 to 0.04532, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 118/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.04532 to 0.04531, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 119/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.04531 to 0.04530, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 120/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.04530 to 0.04529, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 121/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.04529 to 0.04529, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 122/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.04529 to 0.04528, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 123/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.04528 to 0.04527, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 124/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.04527 to 0.04526, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 125/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.04526 to 0.04525, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 126/600\n",
      "1454/1454 [==============================] - 0s 59us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.04525 to 0.04525, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 127/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.04525 to 0.04524, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.04524 to 0.04523, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 129/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.04523 to 0.04523, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 130/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.04523 to 0.04522, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 131/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.04522 to 0.04521, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 132/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.04521 to 0.04521, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 133/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.04521 to 0.04520, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 134/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.04520 to 0.04519, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 135/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.04519 to 0.04518, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 136/600\n",
      "1454/1454 [==============================] - 0s 71us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.04518 to 0.04518, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 137/600\n",
      "1454/1454 [==============================] - 0s 36us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.04518 to 0.04517, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 138/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.04517 to 0.04516, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 139/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.04516 to 0.04516, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 140/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.04516 to 0.04515, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 141/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.04515 to 0.04515, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 142/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.04515 to 0.04514, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 143/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.04514 to 0.04513, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 144/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.04513 to 0.04513, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 145/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.04513 to 0.04512, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 146/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.04512 to 0.04511, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 147/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.04511 to 0.04511, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 148/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.04511 to 0.04510, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 149/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.04510 to 0.04509, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 150/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.04509 to 0.04508, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 151/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.04508 to 0.04508, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 152/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.04508 to 0.04507, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 153/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.04507 to 0.04506, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 154/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.04506 to 0.04505, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 155/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.04505 to 0.04504, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 156/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.04504 to 0.04504, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 157/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.04504 to 0.04503, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 158/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.04503 to 0.04502, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 159/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.04502 to 0.04502, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 160/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.04502 to 0.04501, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 161/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.04501 to 0.04500, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 162/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.04500 to 0.04500, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 163/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.04500 to 0.04499, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 164/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.04499 to 0.04498, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 165/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.04498 to 0.04498, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 166/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.04498 to 0.04497, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 167/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.04497 to 0.04497, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 168/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.04497 to 0.04496, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 169/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.04496 to 0.04496, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 170/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.04496 to 0.04495, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 171/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.04495 to 0.04494, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 172/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.04494 to 0.04494, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 173/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.04494 to 0.04493, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 174/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.04493 to 0.04493, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 175/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.04493 to 0.04492, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 176/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.04492 to 0.04491, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 177/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.04491 to 0.04491, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 178/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.04491 to 0.04490, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 179/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.04490 to 0.04489, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 180/600\n",
      "1454/1454 [==============================] - 0s 35us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.04489 to 0.04489, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 181/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.04489 to 0.04488, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 182/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.04488 to 0.04488, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 183/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.04488 to 0.04487, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 184/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.04487 to 0.04487, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 185/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.04487 to 0.04486, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 186/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.04486 to 0.04486, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 187/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.04486 to 0.04485, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 188/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.04485 to 0.04484, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 189/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.04484 to 0.04484, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 190/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.04484 to 0.04483, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 191/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.04483 to 0.04483, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 192/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.04483 to 0.04482, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 193/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.04482 to 0.04482, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 194/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.04482 to 0.04481, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 195/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.04481 to 0.04480, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 196/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.04480 to 0.04480, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 197/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.04480 to 0.04479, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 198/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.04479 to 0.04479, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 199/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.04479 to 0.04478, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 200/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.04478 to 0.04478, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 201/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.04478 to 0.04477, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 202/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.04477 to 0.04477, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 203/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.04477 to 0.04476, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 204/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.04476 to 0.04476, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 205/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.04476 to 0.04475, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 206/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.04475 to 0.04475, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 207/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.04475 to 0.04474, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 208/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.04474 to 0.04474, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 209/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.04474 to 0.04473, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 210/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.04473 to 0.04473, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 211/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.04473 to 0.04472, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 212/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.04472 to 0.04472, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 213/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.04472 to 0.04472, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 214/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.04472 to 0.04471, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 215/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.04471 to 0.04471, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 216/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.04471 to 0.04470, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 217/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.04470 to 0.04470, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 218/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.04470 to 0.04469, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 219/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.04469 to 0.04469, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 220/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.04469 to 0.04469, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 221/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.04469 to 0.04468, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 222/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.04468 to 0.04468, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 223/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.04468 to 0.04468, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 224/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.04468 to 0.04467, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 225/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.04467 to 0.04467, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 226/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.04467 to 0.04467, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 227/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.04467 to 0.04466, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 228/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.04466 to 0.04466, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 229/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.04466 to 0.04465, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 230/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.04465 to 0.04465, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 231/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.04465 to 0.04465, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 232/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.04465 to 0.04464, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 233/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.04464 to 0.04464, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 234/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.04464 to 0.04464, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 235/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.04464 to 0.04464, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 236/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.04464 to 0.04463, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 237/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.04463 to 0.04463, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 238/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.04463 to 0.04463, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 239/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.04463 to 0.04463, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 240/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.04463 to 0.04462, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 241/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.04462 to 0.04462, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 242/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.04462 to 0.04462, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 243/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.04462 to 0.04461, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 244/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.04461 to 0.04461, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 245/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.04461 to 0.04461, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 246/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.04461 to 0.04461, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 247/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.04461 to 0.04460, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 248/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.04460 to 0.04460, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 249/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.04460 to 0.04460, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 250/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.04460 to 0.04459, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 251/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.04459 to 0.04459, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 252/600\n",
      "1454/1454 [==============================] - 0s 63us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.04459 to 0.04459, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 253/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.04459 to 0.04458, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 254/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.04458 to 0.04458, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 255/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.04458 to 0.04457, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 256/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.04457 to 0.04457, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 257/600\n",
      "1454/1454 [==============================] - 0s 68us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.04457 to 0.04457, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 258/600\n",
      "1454/1454 [==============================] - 0s 63us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.04457 to 0.04456, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 259/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.04456 to 0.04456, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 260/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.04456 to 0.04456, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 261/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.04456 to 0.04455, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 262/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.04455 to 0.04455, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 263/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.04455 to 0.04455, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 264/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.04455 to 0.04454, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 265/600\n",
      "1454/1454 [==============================] - 0s 70us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.04454 to 0.04454, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 266/600\n",
      "1454/1454 [==============================] - 0s 60us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.04454 to 0.04454, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 267/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.04454 to 0.04453, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 268/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.04453 to 0.04453, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 269/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.04453 to 0.04453, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 270/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.04453 to 0.04453, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 271/600\n",
      "1454/1454 [==============================] - 0s 59us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.04453 to 0.04452, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 272/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.04452 to 0.04452, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 273/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.04452 to 0.04452, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 274/600\n",
      "1454/1454 [==============================] - 0s 68us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.04452 to 0.04451, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 275/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.04451 to 0.04451, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 276/600\n",
      "1454/1454 [==============================] - 0s 59us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.04451 to 0.04451, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 277/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.04451 to 0.04451, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 278/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.04451 to 0.04450, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 279/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.04450 to 0.04450, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 280/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.04450 to 0.04450, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 281/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.04450 to 0.04450, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 282/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.04450 to 0.04449, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 283/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.04449 to 0.04449, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 284/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.04449 to 0.04449, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 285/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.04449 to 0.04448, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 286/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.04448 to 0.04448, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 287/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.04448 to 0.04448, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 288/600\n",
      "1454/1454 [==============================] - 0s 59us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.04448 to 0.04447, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 289/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.04447 to 0.04447, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 290/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.04447 to 0.04447, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 291/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.04447 to 0.04446, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 292/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.04446 to 0.04446, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 293/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.04446 to 0.04446, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 294/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.04446 to 0.04445, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 295/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.04445 to 0.04445, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 296/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.04445 to 0.04445, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 297/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.04445 to 0.04444, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 298/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.04444 to 0.04444, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 299/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.04444 to 0.04443, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 300/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.04443 to 0.04443, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 301/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.04443 to 0.04443, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 302/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.04443 to 0.04442, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 303/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.04442 to 0.04442, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 304/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.04442 to 0.04442, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 305/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00305: val_loss improved from 0.04442 to 0.04441, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 306/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.04441 to 0.04441, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 307/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.04441 to 0.04441, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 308/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.04441 to 0.04441, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 309/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.04441 to 0.04440, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 310/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.04440 to 0.04440, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 311/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.04440 to 0.04440, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 312/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.04440 to 0.04439, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 313/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.04439 to 0.04439, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 314/600\n",
      "1454/1454 [==============================] - 0s 59us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.04439 to 0.04439, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 315/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.04439 to 0.04439, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 316/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.04439 to 0.04438, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 317/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.04438 to 0.04438, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 318/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.04438 to 0.04438, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 319/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.04438 to 0.04438, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 320/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.04438 to 0.04437, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 321/600\n",
      "1454/1454 [==============================] - 0s 62us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.04437 to 0.04437, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 322/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.04437 to 0.04437, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 323/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.04437 to 0.04437, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 324/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.04437 to 0.04436, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 325/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.04436 to 0.04436, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 326/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.04436 to 0.04436, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 327/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.04436 to 0.04436, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 328/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.04436 to 0.04435, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 329/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.04435 to 0.04435, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 330/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.04435 to 0.04435, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 331/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.04435 to 0.04435, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 332/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.04435 to 0.04434, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 333/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.04434 to 0.04434, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 334/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.04434 to 0.04434, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 335/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.04434 to 0.04434, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 336/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.04434 to 0.04434, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 337/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.04434 to 0.04433, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 338/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.04433 to 0.04433, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 339/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.04433 to 0.04433, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 340/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.04433 to 0.04433, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 341/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.04433 to 0.04432, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 342/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.04432 to 0.04432, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 343/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.04432 to 0.04432, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 344/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.04432 to 0.04431, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 345/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00345: val_loss improved from 0.04431 to 0.04431, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 346/600\n",
      "1454/1454 [==============================] - 0s 61us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.04431 to 0.04431, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 347/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.04431 to 0.04431, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 348/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.04431 to 0.04430, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 349/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.04430 to 0.04430, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 350/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.04430 to 0.04430, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 351/600\n",
      "1454/1454 [==============================] - 0s 67us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.04430 to 0.04430, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 352/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.04430 to 0.04429, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 353/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00353: val_loss improved from 0.04429 to 0.04429, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 354/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.04429 to 0.04429, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 355/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.04429 to 0.04429, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 356/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.04429 to 0.04429, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 357/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.04429 to 0.04428, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 358/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.04428 to 0.04428, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 359/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.04428 to 0.04428, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 360/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.04428 to 0.04428, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 361/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00361: val_loss improved from 0.04428 to 0.04427, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 362/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.04427 to 0.04427, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 363/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.04427 to 0.04427, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 364/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.04427 to 0.04427, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 365/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.04427 to 0.04426, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 366/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00366: val_loss improved from 0.04426 to 0.04426, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 367/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.04426 to 0.04426, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 368/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00368: val_loss improved from 0.04426 to 0.04426, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 369/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.04426 to 0.04425, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 370/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.04425 to 0.04425, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 371/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.04425 to 0.04425, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 372/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.04425 to 0.04425, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 373/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.04425 to 0.04424, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 374/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.04424 to 0.04424, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 375/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.04424 to 0.04424, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 376/600\n",
      "1454/1454 [==============================] - 0s 62us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.04424 to 0.04424, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 377/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00377: val_loss improved from 0.04424 to 0.04423, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 378/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.04423 to 0.04423, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 379/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.04423 to 0.04423, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 380/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.04423 to 0.04423, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 381/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.04423 to 0.04423, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 382/600\n",
      "1454/1454 [==============================] - 0s 69us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.04423 to 0.04422, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 383/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00383: val_loss improved from 0.04422 to 0.04422, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 384/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00384: val_loss improved from 0.04422 to 0.04422, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 385/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00385: val_loss improved from 0.04422 to 0.04422, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 386/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00386: val_loss improved from 0.04422 to 0.04421, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 387/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00387: val_loss improved from 0.04421 to 0.04421, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 388/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00388: val_loss improved from 0.04421 to 0.04421, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 389/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00389: val_loss improved from 0.04421 to 0.04420, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 390/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00390: val_loss improved from 0.04420 to 0.04420, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 391/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00391: val_loss improved from 0.04420 to 0.04420, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 392/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00392: val_loss improved from 0.04420 to 0.04419, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 393/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00393: val_loss improved from 0.04419 to 0.04419, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 394/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00394: val_loss improved from 0.04419 to 0.04419, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 395/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.04419 to 0.04419, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 396/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00396: val_loss improved from 0.04419 to 0.04418, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 397/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00397: val_loss improved from 0.04418 to 0.04418, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 398/600\n",
      "1454/1454 [==============================] - 0s 61us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00398: val_loss improved from 0.04418 to 0.04418, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 399/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.04418 to 0.04417, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 400/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00400: val_loss improved from 0.04417 to 0.04417, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 401/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00401: val_loss improved from 0.04417 to 0.04417, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 402/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00402: val_loss improved from 0.04417 to 0.04416, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 403/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00403: val_loss improved from 0.04416 to 0.04416, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 404/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00404: val_loss improved from 0.04416 to 0.04416, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 405/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.04416\n",
      "Epoch 406/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.04416\n",
      "Epoch 407/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00407: val_loss improved from 0.04416 to 0.04416, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 408/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00408: val_loss improved from 0.04416 to 0.04416, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 409/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00409: val_loss improved from 0.04416 to 0.04415, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 410/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00410: val_loss improved from 0.04415 to 0.04415, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 411/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "\n",
      "Epoch 00411: val_loss improved from 0.04415 to 0.04415, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 412/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.04415 to 0.04415, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 413/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00413: val_loss improved from 0.04415 to 0.04415, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 414/600\n",
      "1454/1454 [==============================] - 0s 62us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00414: val_loss improved from 0.04415 to 0.04414, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 415/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00415: val_loss improved from 0.04414 to 0.04414, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 416/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00416: val_loss improved from 0.04414 to 0.04414, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 417/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.04414 to 0.04414, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 418/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00418: val_loss improved from 0.04414 to 0.04414, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 419/600\n",
      "1454/1454 [==============================] - 0s 63us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00419: val_loss improved from 0.04414 to 0.04413, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 420/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00420: val_loss improved from 0.04413 to 0.04413, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 421/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00421: val_loss improved from 0.04413 to 0.04413, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 422/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00422: val_loss improved from 0.04413 to 0.04413, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 423/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.04413 to 0.04413, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 424/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00424: val_loss improved from 0.04413 to 0.04412, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 425/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00425: val_loss improved from 0.04412 to 0.04412, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 426/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00426: val_loss improved from 0.04412 to 0.04412, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 427/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.04412 to 0.04412, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 428/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00428: val_loss improved from 0.04412 to 0.04412, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00429: val_loss improved from 0.04412 to 0.04411, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 430/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00430: val_loss improved from 0.04411 to 0.04411, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 431/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00431: val_loss improved from 0.04411 to 0.04411, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 432/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.04411 to 0.04411, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 433/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00433: val_loss improved from 0.04411 to 0.04410, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 434/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00434: val_loss improved from 0.04410 to 0.04410, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 435/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.04410 to 0.04410, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 436/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00436: val_loss improved from 0.04410 to 0.04410, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 437/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00437: val_loss improved from 0.04410 to 0.04409, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 438/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.04409 to 0.04409, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 439/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00439: val_loss improved from 0.04409 to 0.04409, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 440/600\n",
      "1454/1454 [==============================] - 0s 68us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00440: val_loss improved from 0.04409 to 0.04409, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 441/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00441: val_loss improved from 0.04409 to 0.04408, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 442/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00442: val_loss improved from 0.04408 to 0.04408, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 443/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00443: val_loss improved from 0.04408 to 0.04408, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 444/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00444: val_loss improved from 0.04408 to 0.04408, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 445/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00445: val_loss improved from 0.04408 to 0.04408, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 446/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.04408 to 0.04407, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 447/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.04407 to 0.04407, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 448/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00448: val_loss improved from 0.04407 to 0.04407, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 449/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00449: val_loss improved from 0.04407 to 0.04407, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 450/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00450: val_loss improved from 0.04407 to 0.04406, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 451/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00451: val_loss improved from 0.04406 to 0.04406, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 452/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00452: val_loss improved from 0.04406 to 0.04406, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 453/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00453: val_loss improved from 0.04406 to 0.04406, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 454/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00454: val_loss improved from 0.04406 to 0.04406, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 455/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00455: val_loss improved from 0.04406 to 0.04405, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 456/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00456: val_loss improved from 0.04405 to 0.04405, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 457/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00457: val_loss improved from 0.04405 to 0.04405, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 458/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00458: val_loss improved from 0.04405 to 0.04405, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 459/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00459: val_loss improved from 0.04405 to 0.04404, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 460/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00460: val_loss improved from 0.04404 to 0.04404, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 461/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00461: val_loss improved from 0.04404 to 0.04404, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 462/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00462: val_loss improved from 0.04404 to 0.04404, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 463/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00463: val_loss improved from 0.04404 to 0.04404, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 464/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00464: val_loss improved from 0.04404 to 0.04403, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 465/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00465: val_loss improved from 0.04403 to 0.04403, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 466/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00466: val_loss improved from 0.04403 to 0.04403, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 467/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00467: val_loss improved from 0.04403 to 0.04403, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 468/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00468: val_loss improved from 0.04403 to 0.04403, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 469/600\n",
      "1454/1454 [==============================] - 0s 64us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00469: val_loss improved from 0.04403 to 0.04402, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 470/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00470: val_loss improved from 0.04402 to 0.04402, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 471/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00471: val_loss improved from 0.04402 to 0.04402, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 472/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00472: val_loss improved from 0.04402 to 0.04402, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 473/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.04402 to 0.04402, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 474/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00474: val_loss improved from 0.04402 to 0.04401, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 475/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00475: val_loss improved from 0.04401 to 0.04401, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 476/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00476: val_loss improved from 0.04401 to 0.04401, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 477/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00477: val_loss improved from 0.04401 to 0.04401, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 478/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00478: val_loss improved from 0.04401 to 0.04401, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 479/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454/1454 [==============================] - 0s 62us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00479: val_loss improved from 0.04401 to 0.04400, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 480/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00480: val_loss improved from 0.04400 to 0.04400, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 481/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00481: val_loss improved from 0.04400 to 0.04400, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 482/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00482: val_loss improved from 0.04400 to 0.04400, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 483/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00483: val_loss improved from 0.04400 to 0.04400, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 484/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00484: val_loss improved from 0.04400 to 0.04400, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 485/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00485: val_loss improved from 0.04400 to 0.04399, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 486/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00486: val_loss improved from 0.04399 to 0.04399, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 487/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00487: val_loss improved from 0.04399 to 0.04399, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 488/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00488: val_loss improved from 0.04399 to 0.04399, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 489/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00489: val_loss improved from 0.04399 to 0.04399, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 490/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00490: val_loss improved from 0.04399 to 0.04399, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 491/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00491: val_loss improved from 0.04399 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 492/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00492: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 493/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00493: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 494/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00494: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 495/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00495: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 496/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00496: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 497/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00497: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 498/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00498: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 499/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00499: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 500/600\n",
      "1454/1454 [==============================] - 0s 65us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00500: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 501/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00501: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 502/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00502: val_loss improved from 0.04398 to 0.04398, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 503/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00503: val_loss improved from 0.04398 to 0.04397, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 504/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00504: val_loss improved from 0.04397 to 0.04397, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 505/600\n",
      "1454/1454 [==============================] - 0s 38us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00505: val_loss improved from 0.04397 to 0.04397, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 506/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00506: val_loss improved from 0.04397 to 0.04397, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 507/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00507: val_loss improved from 0.04397 to 0.04397, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 508/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00508: val_loss improved from 0.04397 to 0.04397, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 509/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00509: val_loss improved from 0.04397 to 0.04397, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 510/600\n",
      "1454/1454 [==============================] - 0s 53us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00510: val_loss improved from 0.04397 to 0.04396, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 511/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00511: val_loss improved from 0.04396 to 0.04396, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 512/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00512: val_loss improved from 0.04396 to 0.04396, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 513/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00513: val_loss improved from 0.04396 to 0.04396, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 514/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00514: val_loss improved from 0.04396 to 0.04396, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 515/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00515: val_loss improved from 0.04396 to 0.04396, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 516/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00516: val_loss improved from 0.04396 to 0.04395, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 517/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00517: val_loss improved from 0.04395 to 0.04395, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 518/600\n",
      "1454/1454 [==============================] - 0s 39us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "\n",
      "Epoch 00518: val_loss improved from 0.04395 to 0.04395, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 519/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00519: val_loss improved from 0.04395 to 0.04395, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 520/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00520: val_loss improved from 0.04395 to 0.04395, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 521/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00521: val_loss improved from 0.04395 to 0.04395, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 522/600\n",
      "1454/1454 [==============================] - 0s 67us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00522: val_loss improved from 0.04395 to 0.04394, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 523/600\n",
      "1454/1454 [==============================] - 0s 63us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00523: val_loss improved from 0.04394 to 0.04394, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 524/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00524: val_loss improved from 0.04394 to 0.04394, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 525/600\n",
      "1454/1454 [==============================] - 0s 60us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00525: val_loss improved from 0.04394 to 0.04394, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 526/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00526: val_loss improved from 0.04394 to 0.04394, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 527/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00527: val_loss improved from 0.04394 to 0.04394, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 528/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00528: val_loss improved from 0.04394 to 0.04394, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 529/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454/1454 [==============================] - 0s 59us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00529: val_loss improved from 0.04394 to 0.04393, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 530/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00530: val_loss improved from 0.04393 to 0.04393, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 531/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00531: val_loss improved from 0.04393 to 0.04393, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 532/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00532: val_loss improved from 0.04393 to 0.04393, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 533/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00533: val_loss improved from 0.04393 to 0.04393, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 534/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00534: val_loss improved from 0.04393 to 0.04393, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 535/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00535: val_loss improved from 0.04393 to 0.04393, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 536/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00536: val_loss improved from 0.04393 to 0.04393, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 537/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00537: val_loss improved from 0.04393 to 0.04392, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 538/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00538: val_loss improved from 0.04392 to 0.04392, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 539/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00539: val_loss improved from 0.04392 to 0.04392, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 540/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00540: val_loss improved from 0.04392 to 0.04392, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 541/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00541: val_loss improved from 0.04392 to 0.04392, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 542/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00542: val_loss improved from 0.04392 to 0.04392, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 543/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00543: val_loss improved from 0.04392 to 0.04392, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 544/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00544: val_loss improved from 0.04392 to 0.04392, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 545/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00545: val_loss improved from 0.04392 to 0.04392, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 546/600\n",
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00546: val_loss improved from 0.04392 to 0.04391, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 547/600\n",
      "1454/1454 [==============================] - 0s 43us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00547: val_loss improved from 0.04391 to 0.04391, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 548/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00548: val_loss improved from 0.04391 to 0.04391, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 549/600\n",
      "1454/1454 [==============================] - 0s 61us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00549: val_loss improved from 0.04391 to 0.04391, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 550/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00550: val_loss improved from 0.04391 to 0.04391, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 551/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00551: val_loss improved from 0.04391 to 0.04391, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 552/600\n",
      "1454/1454 [==============================] - 0s 37us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00552: val_loss improved from 0.04391 to 0.04391, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 553/600\n",
      "1454/1454 [==============================] - 0s 71us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00553: val_loss improved from 0.04391 to 0.04390, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 554/600\n",
      "1454/1454 [==============================] - 0s 65us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00554: val_loss improved from 0.04390 to 0.04390, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 555/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00555: val_loss improved from 0.04390 to 0.04390, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 556/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00556: val_loss improved from 0.04390 to 0.04390, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 557/600\n",
      "1454/1454 [==============================] - 0s 42us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00557: val_loss improved from 0.04390 to 0.04390, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 558/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00558: val_loss improved from 0.04390 to 0.04390, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 559/600\n",
      "1454/1454 [==============================] - 0s 62us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00559: val_loss improved from 0.04390 to 0.04390, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 560/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00560: val_loss improved from 0.04390 to 0.04390, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 561/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00561: val_loss improved from 0.04390 to 0.04389, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 562/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00562: val_loss improved from 0.04389 to 0.04389, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 563/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00563: val_loss improved from 0.04389 to 0.04389, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 564/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00564: val_loss improved from 0.04389 to 0.04389, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 565/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00565: val_loss improved from 0.04389 to 0.04389, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 566/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00566: val_loss improved from 0.04389 to 0.04389, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 567/600\n",
      "1454/1454 [==============================] - 0s 51us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00567: val_loss improved from 0.04389 to 0.04389, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 568/600\n",
      "1454/1454 [==============================] - 0s 59us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00568: val_loss improved from 0.04389 to 0.04388, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 569/600\n",
      "1454/1454 [==============================] - 0s 49us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00569: val_loss improved from 0.04388 to 0.04388, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 570/600\n",
      "1454/1454 [==============================] - 0s 66us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00570: val_loss improved from 0.04388 to 0.04388, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 571/600\n",
      "1454/1454 [==============================] - 0s 44us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00571: val_loss improved from 0.04388 to 0.04388, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 572/600\n",
      "1454/1454 [==============================] - 0s 52us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00572: val_loss improved from 0.04388 to 0.04388, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 573/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00573: val_loss improved from 0.04388 to 0.04388, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 574/600\n",
      "1454/1454 [==============================] - 0s 55us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00574: val_loss improved from 0.04388 to 0.04387, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 575/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00575: val_loss improved from 0.04387 to 0.04387, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 576/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00576: val_loss improved from 0.04387 to 0.04387, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 577/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00577: val_loss improved from 0.04387 to 0.04387, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 578/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00578: val_loss improved from 0.04387 to 0.04387, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 579/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454/1454 [==============================] - 0s 48us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00579: val_loss improved from 0.04387 to 0.04387, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 580/600\n",
      "1454/1454 [==============================] - 0s 50us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00580: val_loss improved from 0.04387 to 0.04386, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 581/600\n",
      "1454/1454 [==============================] - 0s 63us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00581: val_loss improved from 0.04386 to 0.04386, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 582/600\n",
      "1454/1454 [==============================] - 0s 64us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00582: val_loss improved from 0.04386 to 0.04386, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 583/600\n",
      "1454/1454 [==============================] - 0s 62us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00583: val_loss improved from 0.04386 to 0.04386, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 584/600\n",
      "1454/1454 [==============================] - 0s 59us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00584: val_loss improved from 0.04386 to 0.04386, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 585/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00585: val_loss improved from 0.04386 to 0.04386, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 586/600\n",
      "1454/1454 [==============================] - 0s 40us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00586: val_loss improved from 0.04386 to 0.04385, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 587/600\n",
      "1454/1454 [==============================] - 0s 47us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00587: val_loss improved from 0.04385 to 0.04385, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 588/600\n",
      "1454/1454 [==============================] - 0s 46us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "\n",
      "Epoch 00588: val_loss improved from 0.04385 to 0.04385, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 589/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00589: val_loss improved from 0.04385 to 0.04385, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 590/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00590: val_loss improved from 0.04385 to 0.04385, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 591/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00591: val_loss improved from 0.04385 to 0.04384, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 592/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00592: val_loss improved from 0.04384 to 0.04384, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 593/600\n",
      "1454/1454 [==============================] - 0s 54us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00593: val_loss improved from 0.04384 to 0.04384, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 594/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00594: val_loss improved from 0.04384 to 0.04384, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 595/600\n",
      "1454/1454 [==============================] - 0s 56us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00595: val_loss improved from 0.04384 to 0.04384, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 596/600\n",
      "1454/1454 [==============================] - 0s 41us/step - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00596: val_loss improved from 0.04384 to 0.04384, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 597/600\n",
      "1454/1454 [==============================] - 0s 45us/step - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00597: val_loss improved from 0.04384 to 0.04383, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 598/600\n",
      "1454/1454 [==============================] - 0s 57us/step - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00598: val_loss improved from 0.04383 to 0.04383, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 599/600\n",
      "1454/1454 [==============================] - 0s 58us/step - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00599: val_loss improved from 0.04383 to 0.04383, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Epoch 600/600\n",
      "1454/1454 [==============================] - 0s 69us/step - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "\n",
      "Epoch 00600: val_loss improved from 0.04383 to 0.04383, saving model to /Users/hmohamed/github/data-research-spring2020/sock-shop/models/best_mlp_model.h5\n",
      "Done..!\n"
     ]
    }
   ],
   "source": [
    "model = build_keras_model()\n",
    "\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "\n",
    "# save the best model only\n",
    "mc = ModelCheckpoint(models_dir + 'best_mlp_model.h5'\n",
    "                     , monitor='val_loss'\n",
    "                     , mode='min' \n",
    "                     , verbose=1\n",
    "                     #, save_weights_only=True\n",
    "                     , save_best_only=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)\n",
    "\n",
    "callbacks = [es, mc, reduce_lr]\n",
    "\n",
    "history = model.fit(x_train, y_train\n",
    "                    , epochs=epochs\n",
    "                    , batch_size=batch_size\n",
    "                    , verbose=1\n",
    "                    , validation_split=validation_split\n",
    "                    , shuffle=False\n",
    "                    , callbacks=callbacks)\n",
    "\n",
    "print (\"Done..!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Validation MSE mean: 0.05  std: (0.04)\n"
     ]
    }
   ],
   "source": [
    "val_loss = history.history['val_loss']\n",
    "val_loss_df = pd.DataFrame(val_loss)\n",
    "\n",
    "print(\"Model Validation MSE mean: %.2f  std: (%.2f)\" % \\\n",
    "      (val_loss_df.mean(), val_loss_df.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.041, Test MSE: 0.046\n"
     ]
    }
   ],
   "source": [
    "# load the saved model\n",
    "saved_model = load_model(models_dir + 'best_mlp_model.h5')\n",
    "\n",
    "# evaluate the model\n",
    "train_loss, train_mse = saved_model.evaluate(x_train, y_train, verbose=0)\n",
    "test_loss, test_mse = saved_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Train MSE: %.3f, Test MSE: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7778242 ],\n",
       "       [0.7299318 ],\n",
       "       [0.8511676 ],\n",
       "       ...,\n",
       "       [0.6298868 ],\n",
       "       [0.8846638 ],\n",
       "       [0.62988687]], dtype=float32)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regressor.fit(x_train, y_train)\n",
    "y_hat = saved_model.predict(x_train)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 (training) = 0.282936349146634\n"
     ]
    }
   ],
   "source": [
    "r2_train = metrics.r2_score(y_train, y_hat)\n",
    "print('R2 (training) = {}'.format(r2_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_train, y_hat)\n",
    "plt.xlabel('Latency', size=18)\n",
    "plt.ylabel(\"predicted Latency\", size=18)\n",
    "#plt.xlim(-2,3)\n",
    "#plt.ylim(-3,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Residual PDF')"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train = y_train.values\n",
    "sns.distplot(y_train - y_hat)\n",
    "plt.title('Residual PDF', size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Residual mean: -0.00 std: 0.27  min; -1.04 max: 0.65\n"
     ]
    }
   ],
   "source": [
    "# evaluation mean_absolute_percentage_error\n",
    "train_error =  y_train - y_hat\n",
    "train_error\n",
    "\n",
    "mean_error = np.mean(train_error)\n",
    "min_error = np.min(train_error)\n",
    "max_error = np.max(train_error)\n",
    "std_error = np.std(train_error)\n",
    "\n",
    "print(\"Train Residual mean: %.2f std: %.2f  min; %.2f max: %.2f\" \\\n",
    "      % (mean_error, std_error, min_error, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6298868 ],\n",
       "       [0.6298868 ],\n",
       "       [0.7837633 ],\n",
       "       [0.4915108 ],\n",
       "       [0.35777903],\n",
       "       [0.8767478 ],\n",
       "       [0.6298868 ],\n",
       "       [0.59473443],\n",
       "       [0.4278926 ],\n",
       "       [0.6298868 ],\n",
       "       [0.6298868 ],\n",
       "       [0.6298868 ],\n",
       "       [0.6298868 ],\n",
       "       [0.8085263 ],\n",
       "       [0.56700754],\n",
       "       [0.57568544],\n",
       "       [0.6298868 ],\n",
       "       [0.76040775],\n",
       "       [0.6298868 ],\n",
       "       [0.5464388 ]], dtype=float32)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_test = saved_model.predict(x_test)\n",
    "y_hat_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test, y_hat_test)\n",
    "plt.xlabel('Latency', size=18)\n",
    "plt.ylabel('Predicted Latency', size=18)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.regplot(x=y_test, y=y_hat_test, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.19</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.63</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.43</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prediction  target\n",
       "0        1.88    1.84\n",
       "1        1.88    2.59\n",
       "2        2.19    2.67\n",
       "3        1.63    1.60\n",
       "4        1.43    1.61"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf = pd.DataFrame(np.exp(y_hat_test), columns=['prediction'])\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "perf['target'] = np.exp(y_test)\n",
    "perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>residual</th>\n",
       "      <th>difference%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>27.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.19</td>\n",
       "      <td>2.67</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>17.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.63</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.43</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>11.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.40</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>6.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.49</td>\n",
       "      <td>35.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.81</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>15.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.53</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>23.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>15.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>30.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.49</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>24.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.71</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>30.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.24</td>\n",
       "      <td>2.62</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>14.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.46</td>\n",
       "      <td>34.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.14</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.83</td>\n",
       "      <td>63.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.25</td>\n",
       "      <td>15.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.24</td>\n",
       "      <td>16.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.91</td>\n",
       "      <td>2.67</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>28.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.20</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>14.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>14.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.56</td>\n",
       "      <td>42.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.66</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.92</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.40</td>\n",
       "      <td>26.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.69</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>32.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.52</td>\n",
       "      <td>37.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>11.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>26.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.58</td>\n",
       "      <td>2.20</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>28.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.44</td>\n",
       "      <td>34.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>28.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.37</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>10.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.57</td>\n",
       "      <td>43.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.66</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.39</td>\n",
       "      <td>2.46</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.06</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.99</td>\n",
       "      <td>2.54</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>21.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>14.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.43</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>22.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.98</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.53</td>\n",
       "      <td>36.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.84</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.19</td>\n",
       "      <td>11.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.68</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.33</td>\n",
       "      <td>24.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.77</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.20</td>\n",
       "      <td>12.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.70</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>30.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.80</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>31.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.26</td>\n",
       "      <td>16.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.36</td>\n",
       "      <td>27.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.27</td>\n",
       "      <td>16.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.74</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.45</td>\n",
       "      <td>34.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.49</td>\n",
       "      <td>1.60</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>6.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.72</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>33.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>29.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.49</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>8.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.42</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>22.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.17</td>\n",
       "      <td>10.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>29.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2.39</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2.24</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>8.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.96</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.28</td>\n",
       "      <td>16.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.81</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.12</td>\n",
       "      <td>7.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>40.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2.32</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>27.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>24.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.48</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.38</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.86</td>\n",
       "      <td>56.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.97</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.28</td>\n",
       "      <td>16.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.60</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.12</td>\n",
       "      <td>6.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.41</td>\n",
       "      <td>28.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.79</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.51</td>\n",
       "      <td>39.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2.41</td>\n",
       "      <td>2.64</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>8.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.80</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.38</td>\n",
       "      <td>26.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.60</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.26</td>\n",
       "      <td>19.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.42</td>\n",
       "      <td>29.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.42</td>\n",
       "      <td>32.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.34</td>\n",
       "      <td>22.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2.48</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.24</td>\n",
       "      <td>14.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2.23</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>17.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.58</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>27.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>27.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2.32</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>14.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2.46</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>7.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.62</td>\n",
       "      <td>2.26</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>28.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.09</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>10.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.46</td>\n",
       "      <td>32.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>40.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.83</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.22</td>\n",
       "      <td>13.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.18</td>\n",
       "      <td>10.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.88</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2.25</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>15.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2.26</td>\n",
       "      <td>2.71</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>16.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>36.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.56</td>\n",
       "      <td>42.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2.41</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>27.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.52</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>25.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2.18</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>69.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.48</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>14.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.16</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>13.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>17.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>15.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>17.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.49</td>\n",
       "      <td>1.69</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>11.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.11</td>\n",
       "      <td>6.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>11.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>30.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.51</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>10.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2.26</td>\n",
       "      <td>2.34</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>8.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2.31</td>\n",
       "      <td>2.56</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>9.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2.11</td>\n",
       "      <td>2.64</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>20.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.39</td>\n",
       "      <td>29.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.33</td>\n",
       "      <td>21.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>13.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.03</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>7.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.79</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.51</td>\n",
       "      <td>39.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.43</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2.26</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>16.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1.99</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.27</td>\n",
       "      <td>15.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.35</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>20.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>11.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.24</td>\n",
       "      <td>14.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.74</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>12.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.58</td>\n",
       "      <td>44.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1.86</td>\n",
       "      <td>2.12</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.70</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2.27</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>13.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.97</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>4.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1.79</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.18</td>\n",
       "      <td>11.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.12</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>11.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>24.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.91</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.60</td>\n",
       "      <td>45.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2.61</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.47</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>24.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.29</td>\n",
       "      <td>18.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2.70</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>19.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>16.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>23.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.45</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.07</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>9.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>29.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>23.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.59</td>\n",
       "      <td>45.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.12</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2.04</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>28.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2.75</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>20.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.48</td>\n",
       "      <td>33.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.77</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.48</td>\n",
       "      <td>36.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.93</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.38</td>\n",
       "      <td>24.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>30.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>5.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.51</td>\n",
       "      <td>37.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>20.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>19.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.18</td>\n",
       "      <td>10.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>12.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.51</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>13.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2.30</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>38.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2.92</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.26</td>\n",
       "      <td>9.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.24</td>\n",
       "      <td>17.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.04</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.28</td>\n",
       "      <td>17.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>23.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2.78</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.38</td>\n",
       "      <td>15.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.22</td>\n",
       "      <td>13.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.07</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>9.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.57</td>\n",
       "      <td>43.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.93</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>16.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.38</td>\n",
       "      <td>25.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>11.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>27.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1.92</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.24</td>\n",
       "      <td>14.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>11.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2.35</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>11.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.56</td>\n",
       "      <td>42.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.42</td>\n",
       "      <td>29.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.05</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>8.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2.52</td>\n",
       "      <td>2.67</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2.70</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2.30</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.77</td>\n",
       "      <td>50.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  target  residual  difference%\n",
       "0          1.88    1.84      0.04         2.04\n",
       "1          1.88    2.59     -0.71        27.55\n",
       "2          2.19    2.67     -0.48        17.94\n",
       "3          1.63    1.60      0.04         2.40\n",
       "4          1.43    1.61     -0.18        11.30\n",
       "5          2.40    2.57     -0.17         6.64\n",
       "6          1.88    1.39      0.49        35.15\n",
       "7          1.81    2.15     -0.33        15.52\n",
       "8          1.53    2.01     -0.47        23.50\n",
       "9          1.88    2.21     -0.33        15.14\n",
       "10         1.88    2.70     -0.82        30.49\n",
       "11         1.88    2.49     -0.61        24.49\n",
       "12         1.88    2.71     -0.84        30.83\n",
       "13         2.24    2.62     -0.37        14.31\n",
       "14         1.76    1.31      0.46        34.96\n",
       "15         1.78    1.72      0.06         3.31\n",
       "16         1.88    1.71      0.17         9.64\n",
       "17         2.14    1.31      0.83        63.05\n",
       "18         1.88    1.63      0.25        15.39\n",
       "19         1.73    1.48      0.24        16.50\n",
       "20         1.91    2.67     -0.76        28.43\n",
       "21         1.88    2.20     -0.32        14.73\n",
       "22         1.88    2.18     -0.31        14.02\n",
       "23         1.88    1.32      0.56        42.63\n",
       "24         2.66    2.63      0.03         1.22\n",
       "25         1.92    1.52      0.40        26.05\n",
       "26         1.69    2.48     -0.79        32.02\n",
       "27         1.88    1.36      0.52        37.89\n",
       "28         1.88    1.69      0.19        11.32\n",
       "29         1.88    2.57     -0.69        26.90\n",
       "30         1.58    2.20     -0.62        28.17\n",
       "31         1.73    1.29      0.44        34.05\n",
       "32         1.88    2.63     -0.75        28.65\n",
       "33         2.37    2.63     -0.27        10.08\n",
       "34         1.88    1.30      0.57        43.92\n",
       "35         1.66    1.58      0.08         4.86\n",
       "36         2.39    2.46     -0.06         2.49\n",
       "37         2.06    1.95      0.12         5.98\n",
       "38         1.99    2.54     -0.55        21.70\n",
       "39         1.88    2.21     -0.33        14.91\n",
       "40         1.88    2.43     -0.55        22.69\n",
       "41         1.98    1.45      0.53        36.49\n",
       "42         1.84    1.65      0.19        11.54\n",
       "43         1.68    1.35      0.33        24.12\n",
       "44         1.77    1.57      0.20        12.87\n",
       "45         1.70    1.30      0.40        30.94\n",
       "46         1.80    1.73      0.08         4.61\n",
       "47         1.88    1.43      0.45        31.75\n",
       "48         1.88    1.61      0.26        16.27\n",
       "49         1.65    1.29      0.36        27.60\n",
       "50         1.88    1.61      0.27        16.75\n",
       "51         1.74    1.29      0.45        34.73\n",
       "52         1.88    1.92     -0.04         2.03\n",
       "53         1.49    1.60     -0.11         6.83\n",
       "54         1.72    2.59     -0.86        33.39\n",
       "55         1.88    2.66     -0.78        29.38\n",
       "56         1.88    1.85      0.02         1.34\n",
       "57         1.49    1.63     -0.14         8.66\n",
       "58         1.88    2.42     -0.54        22.48\n",
       "59         1.78    1.60      0.17        10.78\n",
       "60         1.88    2.65     -0.77        29.04\n",
       "61         2.39    2.31      0.08         3.50\n",
       "62         2.24    2.45     -0.21         8.44\n",
       "63         1.96    1.67      0.28        16.86\n",
       "64         1.81    1.69      0.12         7.19\n",
       "65         1.88    1.34      0.54        40.29\n",
       "66         2.32    2.65     -0.33        12.36\n",
       "67         1.78    1.40      0.39        27.52\n",
       "68         1.88    1.51      0.37        24.49\n",
       "69         1.48    1.32      0.16        11.98\n",
       "70         2.38    1.53      0.86        56.11\n",
       "71         1.97    1.69      0.28        16.38\n",
       "72         1.60    1.61     -0.01         0.75\n",
       "73         1.88    1.76      0.12         6.72\n",
       "74         1.88    1.46      0.41        28.16\n",
       "75         1.79    1.29      0.51        39.41\n",
       "76         2.41    2.64     -0.23         8.67\n",
       "77         1.80    1.42      0.38        26.79\n",
       "78         1.60    1.34      0.26        19.02\n",
       "79         1.88    1.45      0.42        29.13\n",
       "80         1.72    1.30      0.42        32.39\n",
       "81         1.88    1.54      0.34        22.11\n",
       "82         1.88    1.79      0.09         4.97\n",
       "83         2.48    2.41      0.07         2.72\n",
       "84         1.88    1.64      0.24        14.43\n",
       "85         2.23    2.70     -0.47        17.51\n",
       "86         1.88    2.58     -0.70        27.10\n",
       "87         1.88    1.47      0.40        27.36\n",
       "88         2.32    2.70     -0.38        14.25\n",
       "89         2.46    2.65     -0.19         7.17\n",
       "90         1.88    1.78      0.10         5.33\n",
       "91         1.62    2.26     -0.64        28.30\n",
       "92         1.88    2.09     -0.22        10.38\n",
       "93         1.88    1.42      0.46        32.29\n",
       "94         1.88    1.34      0.54        40.39\n",
       "95         1.83    1.60      0.22        13.82\n",
       "96         1.88    1.69      0.18        10.86\n",
       "97         1.88    1.88     -0.00         0.26\n",
       "98         2.25    2.65     -0.40        15.05\n",
       "99         2.26    2.71     -0.45        16.62\n",
       "100        1.88    1.38      0.50        36.47\n",
       "101        1.88    1.31      0.56        42.83\n",
       "102        2.41    2.21      0.20         9.08\n",
       "103        1.88    1.48      0.40        27.27\n",
       "104        1.88    2.52     -0.65        25.58\n",
       "105        2.18    1.29      0.89        69.36\n",
       "106        1.48    1.29      0.19        14.47\n",
       "107        1.88    2.16     -0.29        13.25\n",
       "108        1.88    2.27     -0.39        17.28\n",
       "109        1.88    2.22     -0.34        15.38\n",
       "110        1.88    2.27     -0.39        17.22\n",
       "111        1.49    1.69     -0.20        11.66\n",
       "112        1.65    1.55      0.11         6.95\n",
       "113        1.88    2.13     -0.25        11.73\n",
       "114        1.88    2.69     -0.82        30.30\n",
       "115        1.51    1.39      0.12         8.52\n",
       "116        1.88    2.10     -0.22        10.45\n",
       "117        2.26    2.34     -0.08         3.35\n",
       "118        1.88    2.05     -0.17         8.38\n",
       "119        2.31    2.56     -0.25         9.93\n",
       "120        2.11    2.64     -0.54        20.32\n",
       "121        1.88    1.80      0.08         4.18\n",
       "122        1.71    1.32      0.39        29.65\n",
       "123        1.72    1.71      0.01         0.58\n",
       "124        1.88    1.54      0.33        21.69\n",
       "125        1.71    1.99     -0.28        13.95\n",
       "126        1.88    1.92     -0.04         1.97\n",
       "127        1.88    1.86      0.02         1.13\n",
       "128        1.88    2.03     -0.15         7.36\n",
       "129        1.79    1.29      0.51        39.23\n",
       "130        1.76    1.57      0.19        12.04\n",
       "131        1.88    2.43     -0.56        22.85\n",
       "132        2.26    2.72     -0.46        16.81\n",
       "133        1.99    1.72      0.27        15.56\n",
       "134        1.88    2.35     -0.47        20.09\n",
       "135        1.88    1.69      0.19        11.16\n",
       "136        1.88    1.64      0.24        14.59\n",
       "137        1.74    2.00     -0.26        12.86\n",
       "138        1.88    1.30      0.58        44.42\n",
       "139        1.86    2.12     -0.26        12.34\n",
       "140        1.70    1.65      0.04         2.62\n",
       "141        1.73    1.69      0.04         2.17\n",
       "142        2.27    2.63     -0.36        13.64\n",
       "143        1.88    1.97     -0.10         4.83\n",
       "144        1.79    1.61      0.18        11.11\n",
       "145        1.88    2.12     -0.24        11.31\n",
       "146        1.88    1.51      0.37        24.32\n",
       "147        1.91    1.31      0.60        45.67\n",
       "148        2.61    2.69     -0.08         2.90\n",
       "149        1.88    2.47     -0.60        24.07\n",
       "150        1.88    1.96     -0.08         4.24\n",
       "151        1.88    1.59      0.29        18.05\n",
       "152        2.70    2.70     -0.01         0.19\n",
       "153        1.88    2.33     -0.45        19.26\n",
       "154        1.88    2.24     -0.36        16.13\n",
       "155        1.88    2.45     -0.57        23.44\n",
       "156        1.45    1.33      0.12         9.14\n",
       "157        1.88    2.07     -0.20         9.51\n",
       "158        1.88    1.44      0.43        29.94\n",
       "159        1.88    2.44     -0.56        23.01\n",
       "160        1.88    1.29      0.59        45.82\n",
       "161        1.88    1.76      0.12         6.58\n",
       "162        2.04    1.59      0.45        28.13\n",
       "163        2.75    2.70      0.06         2.07\n",
       "164        1.88    2.37     -0.49        20.86\n",
       "165        1.88    1.40      0.48        33.95\n",
       "166        1.77    1.30      0.48        36.68\n",
       "167        1.93    1.56      0.38        24.36\n",
       "168        1.88    1.44      0.44        30.39\n",
       "169        1.88    1.99     -0.12         5.82\n",
       "170        1.88    1.37      0.51        37.22\n",
       "171        1.88    2.37     -0.49        20.64\n",
       "172        1.88    2.33     -0.45        19.48\n",
       "173        1.88    1.70      0.18        10.55\n",
       "174        1.88    2.15     -0.27        12.58\n",
       "175        1.88    1.36      0.51        37.60\n",
       "176        2.24    2.59     -0.36        13.73\n",
       "177        2.30    1.66      0.64        38.35\n",
       "178        2.92    2.66      0.26         9.96\n",
       "179        1.60    1.36      0.24        17.61\n",
       "180        1.88    2.04     -0.16         8.00\n",
       "181        1.88    1.60      0.28        17.56\n",
       "182        1.88    2.44     -0.56        23.12\n",
       "183        2.78    2.40      0.38        15.67\n",
       "184        1.88    1.66      0.22        13.25\n",
       "185        1.88    2.07     -0.19         9.39\n",
       "186        1.88    1.31      0.57        43.72\n",
       "187        1.88    1.93     -0.05         2.64\n",
       "188        1.88    2.24     -0.37        16.31\n",
       "189        1.88    1.49      0.38        25.79\n",
       "190        1.88    2.13     -0.25        11.67\n",
       "191        1.88    1.86      0.01         0.78\n",
       "192        1.88    1.48      0.40        27.10\n",
       "193        1.92    1.67      0.24        14.55\n",
       "194        2.36    2.67     -0.31        11.62\n",
       "195        2.35    2.66     -0.32        11.85\n",
       "196        1.88    1.32      0.56        42.54\n",
       "197        1.88    1.45      0.42        29.22\n",
       "198        1.88    2.05     -0.18         8.63\n",
       "199        2.52    2.67     -0.15         5.64\n",
       "200        2.70    2.65      0.05         1.94\n",
       "201        2.30    1.53      0.77        50.66\n",
       "202        1.88    1.78      0.10         5.48"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing mean_absolute_percentage_error\n",
    "perf['residual'] = perf['prediction'] - perf['target']\n",
    "perf['difference%'] = np.absolute(perf['residual'] * 100 / perf['target'])\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>residual</th>\n",
       "      <th>difference%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>203.00</td>\n",
       "      <td>203.00</td>\n",
       "      <td>203.00</td>\n",
       "      <td>203.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.93</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>13.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.43</td>\n",
       "      <td>1.29</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.51</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>9.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.05</td>\n",
       "      <td>15.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.35</td>\n",
       "      <td>27.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.92</td>\n",
       "      <td>2.72</td>\n",
       "      <td>0.89</td>\n",
       "      <td>69.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction  target  residual  difference%\n",
       "count      203.00  203.00    203.00       203.00\n",
       "mean         1.93    1.93      0.00        18.86\n",
       "std          0.26    0.48      0.40        13.02\n",
       "min          1.43    1.29     -0.86         0.19\n",
       "25%          1.88    1.51     -0.31         9.26\n",
       "50%          1.88    1.79      0.05        15.67\n",
       "75%          1.88    2.39      0.35        27.58\n",
       "max          2.92    2.72      0.89        69.36"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>residual</th>\n",
       "      <th>difference%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2.70</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.88</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.60</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.66</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2.70</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2.75</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.63</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.39</td>\n",
       "      <td>2.46</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.70</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.93</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2.48</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2.61</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2.26</td>\n",
       "      <td>2.34</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2.39</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.80</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.97</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>4.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.66</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2.52</td>\n",
       "      <td>2.67</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>5.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.06</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.12</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.40</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>6.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.12</td>\n",
       "      <td>6.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.49</td>\n",
       "      <td>1.60</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>6.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.11</td>\n",
       "      <td>6.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2.46</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>7.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.81</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.12</td>\n",
       "      <td>7.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.03</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>7.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.04</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>8.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2.24</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>8.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.51</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.05</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>8.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.49</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>8.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2.41</td>\n",
       "      <td>2.64</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>8.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2.41</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.45</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.07</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>9.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.07</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>9.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2.31</td>\n",
       "      <td>2.56</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>9.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2.92</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.26</td>\n",
       "      <td>9.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.37</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>10.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.09</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>10.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>10.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.18</td>\n",
       "      <td>10.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.17</td>\n",
       "      <td>10.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.18</td>\n",
       "      <td>10.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1.79</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.18</td>\n",
       "      <td>11.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>11.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.43</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>11.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.12</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>11.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>11.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.84</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.19</td>\n",
       "      <td>11.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>11.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.49</td>\n",
       "      <td>1.69</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>11.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>11.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>11.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2.35</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>11.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.48</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1.86</td>\n",
       "      <td>2.12</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2.32</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>12.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.74</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>12.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.77</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.20</td>\n",
       "      <td>12.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.16</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>13.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.22</td>\n",
       "      <td>13.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2.27</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>13.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>13.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.83</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.22</td>\n",
       "      <td>13.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>13.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>14.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2.32</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>14.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.24</td>\n",
       "      <td>2.62</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>14.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.24</td>\n",
       "      <td>14.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.48</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>14.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1.92</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.24</td>\n",
       "      <td>14.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.24</td>\n",
       "      <td>14.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.20</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>14.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>14.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2.25</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>15.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>15.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>15.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.25</td>\n",
       "      <td>15.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.81</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>15.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1.99</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.27</td>\n",
       "      <td>15.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2.78</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.38</td>\n",
       "      <td>15.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>16.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.26</td>\n",
       "      <td>16.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>16.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.97</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.28</td>\n",
       "      <td>16.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.24</td>\n",
       "      <td>16.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2.26</td>\n",
       "      <td>2.71</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>16.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.27</td>\n",
       "      <td>16.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2.26</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>16.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.96</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.28</td>\n",
       "      <td>16.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>17.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>17.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2.23</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>17.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.28</td>\n",
       "      <td>17.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.24</td>\n",
       "      <td>17.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.19</td>\n",
       "      <td>2.67</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>17.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.29</td>\n",
       "      <td>18.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.60</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.26</td>\n",
       "      <td>19.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>19.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>19.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.35</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>20.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2.11</td>\n",
       "      <td>2.64</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>20.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>20.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>20.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.33</td>\n",
       "      <td>21.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.99</td>\n",
       "      <td>2.54</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>21.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.34</td>\n",
       "      <td>22.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.42</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>22.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.43</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>22.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.43</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>23.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>23.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>23.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.53</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>23.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.47</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>24.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.68</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.33</td>\n",
       "      <td>24.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>24.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.93</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.38</td>\n",
       "      <td>24.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.49</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>24.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>24.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.52</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>25.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.38</td>\n",
       "      <td>25.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.92</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.40</td>\n",
       "      <td>26.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.80</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.38</td>\n",
       "      <td>26.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>26.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>27.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.58</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>27.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>27.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>27.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>27.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>27.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.36</td>\n",
       "      <td>27.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2.04</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>28.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.41</td>\n",
       "      <td>28.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.58</td>\n",
       "      <td>2.20</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>28.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.62</td>\n",
       "      <td>2.26</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>28.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.91</td>\n",
       "      <td>2.67</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>28.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>28.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>29.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.42</td>\n",
       "      <td>29.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.42</td>\n",
       "      <td>29.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>29.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.39</td>\n",
       "      <td>29.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>29.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>30.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>30.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>30.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.88</td>\n",
       "      <td>2.71</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>30.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.70</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>30.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>31.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.69</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>32.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.46</td>\n",
       "      <td>32.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.42</td>\n",
       "      <td>32.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.72</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>33.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.48</td>\n",
       "      <td>33.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.44</td>\n",
       "      <td>34.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.74</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.45</td>\n",
       "      <td>34.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.76</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.46</td>\n",
       "      <td>34.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.49</td>\n",
       "      <td>35.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>36.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.98</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.53</td>\n",
       "      <td>36.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.77</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.48</td>\n",
       "      <td>36.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.51</td>\n",
       "      <td>37.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.51</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.52</td>\n",
       "      <td>37.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2.30</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>38.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.79</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.51</td>\n",
       "      <td>39.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.79</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.51</td>\n",
       "      <td>39.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>40.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>40.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.56</td>\n",
       "      <td>42.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.56</td>\n",
       "      <td>42.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.56</td>\n",
       "      <td>42.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.57</td>\n",
       "      <td>43.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.57</td>\n",
       "      <td>43.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.58</td>\n",
       "      <td>44.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.91</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.60</td>\n",
       "      <td>45.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.59</td>\n",
       "      <td>45.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2.30</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.77</td>\n",
       "      <td>50.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.38</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.86</td>\n",
       "      <td>56.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.14</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.83</td>\n",
       "      <td>63.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2.18</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>69.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  target  residual  difference%\n",
       "152        2.70    2.70     -0.01         0.19\n",
       "97         1.88    1.88     -0.00         0.26\n",
       "123        1.72    1.71      0.01         0.58\n",
       "72         1.60    1.61     -0.01         0.75\n",
       "191        1.88    1.86      0.01         0.78\n",
       "127        1.88    1.86      0.02         1.13\n",
       "24         2.66    2.63      0.03         1.22\n",
       "56         1.88    1.85      0.02         1.34\n",
       "200        2.70    2.65      0.05         1.94\n",
       "126        1.88    1.92     -0.04         1.97\n",
       "52         1.88    1.92     -0.04         2.03\n",
       "0          1.88    1.84      0.04         2.04\n",
       "163        2.75    2.70      0.06         2.07\n",
       "141        1.73    1.69      0.04         2.17\n",
       "3          1.63    1.60      0.04         2.40\n",
       "36         2.39    2.46     -0.06         2.49\n",
       "140        1.70    1.65      0.04         2.62\n",
       "187        1.88    1.93     -0.05         2.64\n",
       "83         2.48    2.41      0.07         2.72\n",
       "148        2.61    2.69     -0.08         2.90\n",
       "15         1.78    1.72      0.06         3.31\n",
       "117        2.26    2.34     -0.08         3.35\n",
       "61         2.39    2.31      0.08         3.50\n",
       "121        1.88    1.80      0.08         4.18\n",
       "150        1.88    1.96     -0.08         4.24\n",
       "46         1.80    1.73      0.08         4.61\n",
       "143        1.88    1.97     -0.10         4.83\n",
       "35         1.66    1.58      0.08         4.86\n",
       "82         1.88    1.79      0.09         4.97\n",
       "90         1.88    1.78      0.10         5.33\n",
       "202        1.88    1.78      0.10         5.48\n",
       "199        2.52    2.67     -0.15         5.64\n",
       "169        1.88    1.99     -0.12         5.82\n",
       "37         2.06    1.95      0.12         5.98\n",
       "161        1.88    1.76      0.12         6.58\n",
       "5          2.40    2.57     -0.17         6.64\n",
       "73         1.88    1.76      0.12         6.72\n",
       "53         1.49    1.60     -0.11         6.83\n",
       "112        1.65    1.55      0.11         6.95\n",
       "89         2.46    2.65     -0.19         7.17\n",
       "64         1.81    1.69      0.12         7.19\n",
       "128        1.88    2.03     -0.15         7.36\n",
       "180        1.88    2.04     -0.16         8.00\n",
       "118        1.88    2.05     -0.17         8.38\n",
       "62         2.24    2.45     -0.21         8.44\n",
       "115        1.51    1.39      0.12         8.52\n",
       "198        1.88    2.05     -0.18         8.63\n",
       "57         1.49    1.63     -0.14         8.66\n",
       "76         2.41    2.64     -0.23         8.67\n",
       "102        2.41    2.21      0.20         9.08\n",
       "156        1.45    1.33      0.12         9.14\n",
       "185        1.88    2.07     -0.19         9.39\n",
       "157        1.88    2.07     -0.20         9.51\n",
       "16         1.88    1.71      0.17         9.64\n",
       "119        2.31    2.56     -0.25         9.93\n",
       "178        2.92    2.66      0.26         9.96\n",
       "33         2.37    2.63     -0.27        10.08\n",
       "92         1.88    2.09     -0.22        10.38\n",
       "116        1.88    2.10     -0.22        10.45\n",
       "173        1.88    1.70      0.18        10.55\n",
       "59         1.78    1.60      0.17        10.78\n",
       "96         1.88    1.69      0.18        10.86\n",
       "144        1.79    1.61      0.18        11.11\n",
       "135        1.88    1.69      0.19        11.16\n",
       "4          1.43    1.61     -0.18        11.30\n",
       "145        1.88    2.12     -0.24        11.31\n",
       "28         1.88    1.69      0.19        11.32\n",
       "42         1.84    1.65      0.19        11.54\n",
       "194        2.36    2.67     -0.31        11.62\n",
       "111        1.49    1.69     -0.20        11.66\n",
       "190        1.88    2.13     -0.25        11.67\n",
       "113        1.88    2.13     -0.25        11.73\n",
       "195        2.35    2.66     -0.32        11.85\n",
       "69         1.48    1.32      0.16        11.98\n",
       "130        1.76    1.57      0.19        12.04\n",
       "139        1.86    2.12     -0.26        12.34\n",
       "66         2.32    2.65     -0.33        12.36\n",
       "174        1.88    2.15     -0.27        12.58\n",
       "137        1.74    2.00     -0.26        12.86\n",
       "44         1.77    1.57      0.20        12.87\n",
       "107        1.88    2.16     -0.29        13.25\n",
       "184        1.88    1.66      0.22        13.25\n",
       "142        2.27    2.63     -0.36        13.64\n",
       "176        2.24    2.59     -0.36        13.73\n",
       "95         1.83    1.60      0.22        13.82\n",
       "125        1.71    1.99     -0.28        13.95\n",
       "22         1.88    2.18     -0.31        14.02\n",
       "88         2.32    2.70     -0.38        14.25\n",
       "13         2.24    2.62     -0.37        14.31\n",
       "84         1.88    1.64      0.24        14.43\n",
       "106        1.48    1.29      0.19        14.47\n",
       "193        1.92    1.67      0.24        14.55\n",
       "136        1.88    1.64      0.24        14.59\n",
       "21         1.88    2.20     -0.32        14.73\n",
       "39         1.88    2.21     -0.33        14.91\n",
       "98         2.25    2.65     -0.40        15.05\n",
       "9          1.88    2.21     -0.33        15.14\n",
       "109        1.88    2.22     -0.34        15.38\n",
       "18         1.88    1.63      0.25        15.39\n",
       "7          1.81    2.15     -0.33        15.52\n",
       "133        1.99    1.72      0.27        15.56\n",
       "183        2.78    2.40      0.38        15.67\n",
       "154        1.88    2.24     -0.36        16.13\n",
       "48         1.88    1.61      0.26        16.27\n",
       "188        1.88    2.24     -0.37        16.31\n",
       "71         1.97    1.69      0.28        16.38\n",
       "19         1.73    1.48      0.24        16.50\n",
       "99         2.26    2.71     -0.45        16.62\n",
       "50         1.88    1.61      0.27        16.75\n",
       "132        2.26    2.72     -0.46        16.81\n",
       "63         1.96    1.67      0.28        16.86\n",
       "110        1.88    2.27     -0.39        17.22\n",
       "108        1.88    2.27     -0.39        17.28\n",
       "85         2.23    2.70     -0.47        17.51\n",
       "181        1.88    1.60      0.28        17.56\n",
       "179        1.60    1.36      0.24        17.61\n",
       "2          2.19    2.67     -0.48        17.94\n",
       "151        1.88    1.59      0.29        18.05\n",
       "78         1.60    1.34      0.26        19.02\n",
       "153        1.88    2.33     -0.45        19.26\n",
       "172        1.88    2.33     -0.45        19.48\n",
       "134        1.88    2.35     -0.47        20.09\n",
       "120        2.11    2.64     -0.54        20.32\n",
       "171        1.88    2.37     -0.49        20.64\n",
       "164        1.88    2.37     -0.49        20.86\n",
       "124        1.88    1.54      0.33        21.69\n",
       "38         1.99    2.54     -0.55        21.70\n",
       "81         1.88    1.54      0.34        22.11\n",
       "58         1.88    2.42     -0.54        22.48\n",
       "40         1.88    2.43     -0.55        22.69\n",
       "131        1.88    2.43     -0.56        22.85\n",
       "159        1.88    2.44     -0.56        23.01\n",
       "182        1.88    2.44     -0.56        23.12\n",
       "155        1.88    2.45     -0.57        23.44\n",
       "8          1.53    2.01     -0.47        23.50\n",
       "149        1.88    2.47     -0.60        24.07\n",
       "43         1.68    1.35      0.33        24.12\n",
       "146        1.88    1.51      0.37        24.32\n",
       "167        1.93    1.56      0.38        24.36\n",
       "11         1.88    2.49     -0.61        24.49\n",
       "68         1.88    1.51      0.37        24.49\n",
       "104        1.88    2.52     -0.65        25.58\n",
       "189        1.88    1.49      0.38        25.79\n",
       "25         1.92    1.52      0.40        26.05\n",
       "77         1.80    1.42      0.38        26.79\n",
       "29         1.88    2.57     -0.69        26.90\n",
       "192        1.88    1.48      0.40        27.10\n",
       "86         1.88    2.58     -0.70        27.10\n",
       "103        1.88    1.48      0.40        27.27\n",
       "87         1.88    1.47      0.40        27.36\n",
       "67         1.78    1.40      0.39        27.52\n",
       "1          1.88    2.59     -0.71        27.55\n",
       "49         1.65    1.29      0.36        27.60\n",
       "162        2.04    1.59      0.45        28.13\n",
       "74         1.88    1.46      0.41        28.16\n",
       "30         1.58    2.20     -0.62        28.17\n",
       "91         1.62    2.26     -0.64        28.30\n",
       "20         1.91    2.67     -0.76        28.43\n",
       "32         1.88    2.63     -0.75        28.65\n",
       "60         1.88    2.65     -0.77        29.04\n",
       "79         1.88    1.45      0.42        29.13\n",
       "197        1.88    1.45      0.42        29.22\n",
       "55         1.88    2.66     -0.78        29.38\n",
       "122        1.71    1.32      0.39        29.65\n",
       "158        1.88    1.44      0.43        29.94\n",
       "114        1.88    2.69     -0.82        30.30\n",
       "168        1.88    1.44      0.44        30.39\n",
       "10         1.88    2.70     -0.82        30.49\n",
       "12         1.88    2.71     -0.84        30.83\n",
       "45         1.70    1.30      0.40        30.94\n",
       "47         1.88    1.43      0.45        31.75\n",
       "26         1.69    2.48     -0.79        32.02\n",
       "93         1.88    1.42      0.46        32.29\n",
       "80         1.72    1.30      0.42        32.39\n",
       "54         1.72    2.59     -0.86        33.39\n",
       "165        1.88    1.40      0.48        33.95\n",
       "31         1.73    1.29      0.44        34.05\n",
       "51         1.74    1.29      0.45        34.73\n",
       "14         1.76    1.31      0.46        34.96\n",
       "6          1.88    1.39      0.49        35.15\n",
       "100        1.88    1.38      0.50        36.47\n",
       "41         1.98    1.45      0.53        36.49\n",
       "166        1.77    1.30      0.48        36.68\n",
       "170        1.88    1.37      0.51        37.22\n",
       "175        1.88    1.36      0.51        37.60\n",
       "27         1.88    1.36      0.52        37.89\n",
       "177        2.30    1.66      0.64        38.35\n",
       "129        1.79    1.29      0.51        39.23\n",
       "75         1.79    1.29      0.51        39.41\n",
       "65         1.88    1.34      0.54        40.29\n",
       "94         1.88    1.34      0.54        40.39\n",
       "196        1.88    1.32      0.56        42.54\n",
       "23         1.88    1.32      0.56        42.63\n",
       "101        1.88    1.31      0.56        42.83\n",
       "186        1.88    1.31      0.57        43.72\n",
       "34         1.88    1.30      0.57        43.92\n",
       "138        1.88    1.30      0.58        44.42\n",
       "147        1.91    1.31      0.60        45.67\n",
       "160        1.88    1.29      0.59        45.82\n",
       "201        2.30    1.53      0.77        50.66\n",
       "70         2.38    1.53      0.86        56.11\n",
       "17         2.14    1.31      0.83        63.05\n",
       "105        2.18    1.29      0.89        69.36"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "perf.sort_values(by = ['difference%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 (testing) = 0.2619290769052268\n"
     ]
    }
   ],
   "source": [
    "r2_test = metrics.r2_score(y_test, y_hat_test)\n",
    "print('R2 (testing) = {}'.format(r2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21433813653503841\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_error(y_test,y_hat_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
