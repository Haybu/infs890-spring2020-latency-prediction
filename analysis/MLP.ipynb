{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.callbacks import EarlyStopping , ReduceLROnPlateau , ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "sns.set(color_codes=True)\n",
    "#sns.set_color_codes()\n",
    "\n",
    "pd.options.display.max_rows = 15\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network structure. \n",
    "# not to include the input layer\n",
    "net_layers = (3,2)\n",
    "\n",
    "epochs=200\n",
    "batch_size=300\n",
    "\n",
    "learning_rate = 1e-3\n",
    "decay = learning_rate / epochs\n",
    "\n",
    "patience=80\n",
    "\n",
    "test_split = 0.1\n",
    "validation_split = 0.2\n",
    "\n",
    "# trials\n",
    "# 11 R2-> -0.12\n",
    "# 11,16,8,4 R2-> 0.37\n",
    "# 11,11,6 R2-> -0.2\n",
    "# 11,8,6 R2->   0.316\n",
    "# 11,11,8,4 R2->  0.37\n",
    "# 11, 6 R2-> 0.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- read data file\n",
    "# 1- read processed file\n",
    "file_dir = '../data/processed-data/'\n",
    "data_file = 'normalized_dataset.csv'\n",
    "#data_file = 'standardized_normalized_dataset.csv'\n",
    "data = pd.read_csv(file_dir + data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ltcy</th>\n",
       "      <th>svc_cpu_use</th>\n",
       "      <th>svc_cpu_thr</th>\n",
       "      <th>svc_net_use</th>\n",
       "      <th>svc_disk_use</th>\n",
       "      <th>system_cpu_use</th>\n",
       "      <th>system_cpu_sat</th>\n",
       "      <th>system_net_use</th>\n",
       "      <th>svc_req_size</th>\n",
       "      <th>svc_resp_size</th>\n",
       "      <th>svc_pods</th>\n",
       "      <th>svc_req_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.332</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.037</td>\n",
       "      <td>16.982</td>\n",
       "      <td>1.591</td>\n",
       "      <td>3.206</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.012</td>\n",
       "      <td>7.000</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.047</td>\n",
       "      <td>20.583</td>\n",
       "      <td>1.608</td>\n",
       "      <td>3.552</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.019</td>\n",
       "      <td>7.000</td>\n",
       "      <td>1.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.469</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.039</td>\n",
       "      <td>19.448</td>\n",
       "      <td>1.390</td>\n",
       "      <td>3.586</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.024</td>\n",
       "      <td>7.000</td>\n",
       "      <td>2.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.490</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.108</td>\n",
       "      <td>17.319</td>\n",
       "      <td>1.730</td>\n",
       "      <td>3.512</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.022</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.133</td>\n",
       "      <td>16.650</td>\n",
       "      <td>1.917</td>\n",
       "      <td>3.449</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.023</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ltcy  svc_cpu_use  svc_cpu_thr  svc_net_use  svc_disk_use  system_cpu_use  \\\n",
       "0 0.332        0.557        0.332        0.325         0.037          16.982   \n",
       "1 0.400        0.616        0.300        0.351         0.047          20.583   \n",
       "2 0.469        0.608        0.316        0.362         0.039          19.448   \n",
       "3 0.490        0.624        0.300        0.362         0.108          17.319   \n",
       "4 0.500        0.608        0.316        0.374         0.133          16.650   \n",
       "\n",
       "   system_cpu_sat  system_net_use  svc_req_size  svc_resp_size  svc_pods  \\\n",
       "0           1.591           3.206         0.002          0.012     7.000   \n",
       "1           1.608           3.552         0.003          0.019     7.000   \n",
       "2           1.390           3.586         0.003          0.024     7.000   \n",
       "3           1.730           3.512         0.003          0.022     6.000   \n",
       "4           1.917           3.449         0.003          0.023     3.000   \n",
       "\n",
       "   svc_req_rate  \n",
       "0         0.980  \n",
       "1         1.620  \n",
       "2         2.180  \n",
       "3         2.130  \n",
       "4         2.220  "
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1584, 12)"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 11 features\n"
     ]
    }
   ],
   "source": [
    "targets = data['ltcy']\n",
    "inputs = data.drop(['ltcy'], axis=1)\n",
    "\n",
    "n_features = inputs.values.shape[1]\n",
    "print(\"there are {} features\".format(n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 features selected\n",
      "columns selected are svc_cpu_thr, system_cpu_use, system_cpu_sat, svc_req_rate\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    sfm = SelectFromModel(\n",
    "                RandomForestRegressor(n_jobs=-1, max_depth=10, n_estimators=15),\n",
    "                threshold='0.8*mean')\n",
    "\n",
    "    selectedFeatures = sfm.fit(inputs, targets).transform(inputs)\n",
    "    print('{} features selected'.format(selectedFeatures[1].shape[0]))\n",
    "\n",
    "    feature_list = inputs.columns[sfm.get_support()]\n",
    "    features = ''\n",
    "    features = ', '.join(feature_list)\n",
    "    \n",
    "    print(\"columns selected are {}\".format(features))\n",
    "\n",
    "    inputs = inputs[feature_list]\n",
    "    inputs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = selectedFeatures[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (1425,) , y_test (159,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(inputs, targets, test_size=test_split, shuffle=True, random_state=365)\n",
    "\n",
    "print(\"y_train {} , y_test {}\".format(y_train.shape, y_test.shape))\n",
    "\n",
    "# for better convergence and result scale target to values between 0 - 1\n",
    "y_train_max = y_train.max()\n",
    "y_test_max = y_test.max()\n",
    "\n",
    "y_train = y_train / y_train_max\n",
    "y_test = y_test / y_test_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)  # fit on training data only\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(nodes = net_layers):   # this does not work with KerasRegressor. interesting\n",
    "    # create model\n",
    "    model = Sequential()  \n",
    "    \n",
    "    #if isinstance(nodes, int):\n",
    "    if not nodes:\n",
    "        model.add(Dense(1, input_dim=n_inputs, kernel_initializer='normal'))\n",
    "    \n",
    "    else:\n",
    "        model.add(Dense(nodes[1], input_dim=n_inputs, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "        layer = 0\n",
    "        while layer < len(nodes):\n",
    "            model.add(Dense(nodes[layer], kernel_initializer='normal', activation='relu'))\n",
    "            layer = layer + 1\n",
    "        \n",
    "        #model.add(Dense(1, kernel_initializer='normal', activation='linear')) \n",
    "        model.add(Dense(1, kernel_initializer='normal'))  \n",
    "    \n",
    "    adam = Adam(lr=learning_rate, decay=decay)\n",
    "    \n",
    "    # or loss= 'mean_absolute_percentage_error'\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mse'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1140 samples, validate on 285 samples\n",
      "Epoch 1/200\n",
      "1140/1140 [==============================] - 6s 5ms/step - loss: 0.4283 - mean_squared_error: 0.4283 - val_loss: 0.4397 - val_mean_squared_error: 0.4397\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43968, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 2/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.4232 - mean_squared_error: 0.4232 - val_loss: 0.4346 - val_mean_squared_error: 0.4346\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43968 to 0.43456, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 3/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.4182 - mean_squared_error: 0.4182 - val_loss: 0.4295 - val_mean_squared_error: 0.4295\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43456 to 0.42948, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 4/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.4132 - mean_squared_error: 0.4132 - val_loss: 0.4244 - val_mean_squared_error: 0.4244\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.42948 to 0.42443, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 5/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.4082 - mean_squared_error: 0.4082 - val_loss: 0.4194 - val_mean_squared_error: 0.4194\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42443 to 0.41943, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 6/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.4033 - mean_squared_error: 0.4033 - val_loss: 0.4145 - val_mean_squared_error: 0.4145\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.41943 to 0.41446, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 7/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.3985 - mean_squared_error: 0.3985 - val_loss: 0.4095 - val_mean_squared_error: 0.4095\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.41446 to 0.40954, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 8/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.3936 - mean_squared_error: 0.3936 - val_loss: 0.4047 - val_mean_squared_error: 0.4047\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.40954 to 0.40467, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 9/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.3888 - mean_squared_error: 0.3888 - val_loss: 0.3998 - val_mean_squared_error: 0.3998\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.40467 to 0.39983, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 10/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3841 - mean_squared_error: 0.3841 - val_loss: 0.3950 - val_mean_squared_error: 0.3950\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.39983 to 0.39505, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 11/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3794 - mean_squared_error: 0.3794 - val_loss: 0.3903 - val_mean_squared_error: 0.3903\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.39505 to 0.39030, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 12/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.3747 - mean_squared_error: 0.3747 - val_loss: 0.3856 - val_mean_squared_error: 0.3856\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.39030 to 0.38561, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 13/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.3701 - mean_squared_error: 0.3701 - val_loss: 0.3810 - val_mean_squared_error: 0.3810\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.38561 to 0.38096, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 14/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.3656 - mean_squared_error: 0.3656 - val_loss: 0.3763 - val_mean_squared_error: 0.3763\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.38096 to 0.37635, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 15/200\n",
      "1140/1140 [==============================] - 0s 22us/step - loss: 0.3610 - mean_squared_error: 0.3610 - val_loss: 0.3718 - val_mean_squared_error: 0.3718\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.37635 to 0.37179, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 16/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3673 - val_mean_squared_error: 0.3673\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.37179 to 0.36727, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 17/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3521 - mean_squared_error: 0.3521 - val_loss: 0.3628 - val_mean_squared_error: 0.3628\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.36727 to 0.36279, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 18/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3478 - mean_squared_error: 0.3478 - val_loss: 0.3584 - val_mean_squared_error: 0.3584\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.36279 to 0.35837, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 19/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3434 - mean_squared_error: 0.3434 - val_loss: 0.3540 - val_mean_squared_error: 0.3540\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.35837 to 0.35399, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 20/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3391 - mean_squared_error: 0.3391 - val_loss: 0.3496 - val_mean_squared_error: 0.3496\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.35399 to 0.34965, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 21/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3349 - mean_squared_error: 0.3349 - val_loss: 0.3454 - val_mean_squared_error: 0.3454\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.34965 to 0.34535, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 22/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3307 - mean_squared_error: 0.3307 - val_loss: 0.3411 - val_mean_squared_error: 0.3411\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.34535 to 0.34110, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 23/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.3265 - mean_squared_error: 0.3265 - val_loss: 0.3369 - val_mean_squared_error: 0.3369\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.34110 to 0.33690, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 24/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3224 - mean_squared_error: 0.3224 - val_loss: 0.3327 - val_mean_squared_error: 0.3327\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.33690 to 0.33273, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 25/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3183 - mean_squared_error: 0.3183 - val_loss: 0.3286 - val_mean_squared_error: 0.3286\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.33273 to 0.32861, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 26/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.3143 - mean_squared_error: 0.3143 - val_loss: 0.3245 - val_mean_squared_error: 0.3245\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.32861 to 0.32453, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 27/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.3103 - mean_squared_error: 0.3103 - val_loss: 0.3205 - val_mean_squared_error: 0.3205\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.32453 to 0.32050, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 28/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.3063 - mean_squared_error: 0.3063 - val_loss: 0.3165 - val_mean_squared_error: 0.3165\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.32050 to 0.31650, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 29/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.3024 - mean_squared_error: 0.3024 - val_loss: 0.3125 - val_mean_squared_error: 0.3125\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.31650 to 0.31255, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 30/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.2985 - mean_squared_error: 0.2985 - val_loss: 0.3086 - val_mean_squared_error: 0.3086\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.31255 to 0.30864, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 31/200\n",
      "1140/1140 [==============================] - 0s 14us/step - loss: 0.2947 - mean_squared_error: 0.2947 - val_loss: 0.3048 - val_mean_squared_error: 0.3048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss improved from 0.30864 to 0.30477, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 32/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.2909 - mean_squared_error: 0.2909 - val_loss: 0.3009 - val_mean_squared_error: 0.3009\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.30477 to 0.30094, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 33/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.2872 - mean_squared_error: 0.2872 - val_loss: 0.2972 - val_mean_squared_error: 0.2972\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.30094 to 0.29715, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 34/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.2835 - mean_squared_error: 0.2835 - val_loss: 0.2934 - val_mean_squared_error: 0.2934\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.29715 to 0.29340, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 35/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.2798 - mean_squared_error: 0.2798 - val_loss: 0.2897 - val_mean_squared_error: 0.2897\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.29340 to 0.28970, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 36/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.2762 - mean_squared_error: 0.2762 - val_loss: 0.2860 - val_mean_squared_error: 0.2860\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.28970 to 0.28603, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 37/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.2726 - mean_squared_error: 0.2726 - val_loss: 0.2824 - val_mean_squared_error: 0.2824\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.28603 to 0.28240, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 38/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.2690 - mean_squared_error: 0.2690 - val_loss: 0.2788 - val_mean_squared_error: 0.2788\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.28240 to 0.27881, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 39/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.2655 - mean_squared_error: 0.2655 - val_loss: 0.2753 - val_mean_squared_error: 0.2753\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.27881 to 0.27526, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 40/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2620 - mean_squared_error: 0.2620 - val_loss: 0.2717 - val_mean_squared_error: 0.2717\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.27526 to 0.27175, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 41/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.2586 - mean_squared_error: 0.2586 - val_loss: 0.2683 - val_mean_squared_error: 0.2683\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.27175 to 0.26828, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 42/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.2552 - mean_squared_error: 0.2552 - val_loss: 0.2648 - val_mean_squared_error: 0.2648\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.26828 to 0.26484, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 43/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.2519 - mean_squared_error: 0.2519 - val_loss: 0.2614 - val_mean_squared_error: 0.2614\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.26484 to 0.26145, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 44/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2485 - mean_squared_error: 0.2485 - val_loss: 0.2581 - val_mean_squared_error: 0.2581\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.26145 to 0.25809, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 45/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2453 - mean_squared_error: 0.2453 - val_loss: 0.2548 - val_mean_squared_error: 0.2548\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.25809 to 0.25477, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 46/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.2420 - mean_squared_error: 0.2420 - val_loss: 0.2515 - val_mean_squared_error: 0.2515\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.25477 to 0.25148, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 47/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2388 - mean_squared_error: 0.2388 - val_loss: 0.2482 - val_mean_squared_error: 0.2482\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.25148 to 0.24823, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 48/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.2356 - mean_squared_error: 0.2356 - val_loss: 0.2450 - val_mean_squared_error: 0.2450\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.24823 to 0.24502, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 49/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2325 - mean_squared_error: 0.2325 - val_loss: 0.2418 - val_mean_squared_error: 0.2418\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.24502 to 0.24185, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 50/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.2294 - mean_squared_error: 0.2294 - val_loss: 0.2387 - val_mean_squared_error: 0.2387\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.24185 to 0.23871, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 51/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2263 - mean_squared_error: 0.2263 - val_loss: 0.2356 - val_mean_squared_error: 0.2356\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.23871 to 0.23561, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 52/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.2233 - mean_squared_error: 0.2233 - val_loss: 0.2325 - val_mean_squared_error: 0.2325\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.23561 to 0.23254, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 53/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2203 - mean_squared_error: 0.2203 - val_loss: 0.2295 - val_mean_squared_error: 0.2295\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.23254 to 0.22951, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 54/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.2174 - mean_squared_error: 0.2174 - val_loss: 0.2265 - val_mean_squared_error: 0.2265\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.22951 to 0.22651, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 55/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2144 - mean_squared_error: 0.2144 - val_loss: 0.2235 - val_mean_squared_error: 0.2235\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.22651 to 0.22355, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 56/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2116 - mean_squared_error: 0.2116 - val_loss: 0.2206 - val_mean_squared_error: 0.2206\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.22355 to 0.22062, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 57/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.2087 - mean_squared_error: 0.2087 - val_loss: 0.2177 - val_mean_squared_error: 0.2177\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.22062 to 0.21773, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 58/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2059 - mean_squared_error: 0.2059 - val_loss: 0.2149 - val_mean_squared_error: 0.2149\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.21773 to 0.21487, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 59/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2031 - mean_squared_error: 0.2031 - val_loss: 0.2120 - val_mean_squared_error: 0.2120\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.21487 to 0.21204, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 60/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.2003 - mean_squared_error: 0.2003 - val_loss: 0.2092 - val_mean_squared_error: 0.2092\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.21204 to 0.20925, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 61/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.1976 - mean_squared_error: 0.1976 - val_loss: 0.2065 - val_mean_squared_error: 0.2065\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.20925 to 0.20649, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.1949 - mean_squared_error: 0.1949 - val_loss: 0.2038 - val_mean_squared_error: 0.2038\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.20649 to 0.20376, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 63/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1923 - mean_squared_error: 0.1923 - val_loss: 0.2011 - val_mean_squared_error: 0.2011\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.20376 to 0.20107, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 64/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1897 - mean_squared_error: 0.1897 - val_loss: 0.1984 - val_mean_squared_error: 0.1984\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.20107 to 0.19841, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 65/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1871 - mean_squared_error: 0.1871 - val_loss: 0.1958 - val_mean_squared_error: 0.1958\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.19841 to 0.19578, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 66/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.1845 - mean_squared_error: 0.1845 - val_loss: 0.1932 - val_mean_squared_error: 0.1932\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.19578 to 0.19318, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 67/200\n",
      "1140/1140 [==============================] - 0s 14us/step - loss: 0.1820 - mean_squared_error: 0.1820 - val_loss: 0.1906 - val_mean_squared_error: 0.1906\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.19318 to 0.19062, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 68/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1795 - mean_squared_error: 0.1795 - val_loss: 0.1881 - val_mean_squared_error: 0.1881\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.19062 to 0.18808, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 69/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.1770 - mean_squared_error: 0.1770 - val_loss: 0.1856 - val_mean_squared_error: 0.1856\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.18808 to 0.18558, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 70/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1746 - mean_squared_error: 0.1746 - val_loss: 0.1831 - val_mean_squared_error: 0.1831\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.18558 to 0.18311, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 71/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1722 - mean_squared_error: 0.1722 - val_loss: 0.1807 - val_mean_squared_error: 0.1807\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.18311 to 0.18067, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 72/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1698 - mean_squared_error: 0.1698 - val_loss: 0.1783 - val_mean_squared_error: 0.1783\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.18067 to 0.17825, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 73/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1675 - mean_squared_error: 0.1675 - val_loss: 0.1759 - val_mean_squared_error: 0.1759\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.17825 to 0.17587, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 74/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1651 - mean_squared_error: 0.1651 - val_loss: 0.1735 - val_mean_squared_error: 0.1735\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.17587 to 0.17352, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 75/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1629 - mean_squared_error: 0.1629 - val_loss: 0.1712 - val_mean_squared_error: 0.1712\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.17352 to 0.17120, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 76/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1606 - mean_squared_error: 0.1606 - val_loss: 0.1689 - val_mean_squared_error: 0.1689\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.17120 to 0.16891, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 77/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1584 - mean_squared_error: 0.1584 - val_loss: 0.1666 - val_mean_squared_error: 0.1666\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.16891 to 0.16664, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 78/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1562 - mean_squared_error: 0.1562 - val_loss: 0.1644 - val_mean_squared_error: 0.1644\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.16664 to 0.16441, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 79/200\n",
      "1140/1140 [==============================] - 0s 14us/step - loss: 0.1540 - mean_squared_error: 0.1540 - val_loss: 0.1622 - val_mean_squared_error: 0.1622\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.16441 to 0.16220, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 80/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1519 - mean_squared_error: 0.1519 - val_loss: 0.1600 - val_mean_squared_error: 0.1600\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.16220 to 0.16002, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 81/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1498 - mean_squared_error: 0.1498 - val_loss: 0.1579 - val_mean_squared_error: 0.1579\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.16002 to 0.15787, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 82/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.1477 - mean_squared_error: 0.1477 - val_loss: 0.1558 - val_mean_squared_error: 0.1558\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.15787 to 0.15575, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 83/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.1456 - mean_squared_error: 0.1456 - val_loss: 0.1537 - val_mean_squared_error: 0.1537\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.15575 to 0.15366, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 84/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1436 - mean_squared_error: 0.1436 - val_loss: 0.1516 - val_mean_squared_error: 0.1516\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.15366 to 0.15159, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 85/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1416 - mean_squared_error: 0.1416 - val_loss: 0.1496 - val_mean_squared_error: 0.1496\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.15159 to 0.14955, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 86/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1396 - mean_squared_error: 0.1396 - val_loss: 0.1475 - val_mean_squared_error: 0.1475\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.14955 to 0.14754, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 87/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.1377 - mean_squared_error: 0.1377 - val_loss: 0.1456 - val_mean_squared_error: 0.1456\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.14754 to 0.14555, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 88/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1357 - mean_squared_error: 0.1357 - val_loss: 0.1436 - val_mean_squared_error: 0.1436\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.14555 to 0.14359, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 89/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.1338 - mean_squared_error: 0.1338 - val_loss: 0.1417 - val_mean_squared_error: 0.1417\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.14359 to 0.14166, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 90/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.1320 - mean_squared_error: 0.1320 - val_loss: 0.1397 - val_mean_squared_error: 0.1397\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.14166 to 0.13975, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 91/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.1301 - mean_squared_error: 0.1301 - val_loss: 0.1379 - val_mean_squared_error: 0.1379\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.13975 to 0.13787, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 92/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1283 - mean_squared_error: 0.1283 - val_loss: 0.1360 - val_mean_squared_error: 0.1360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00092: val_loss improved from 0.13787 to 0.13601, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 93/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.1265 - mean_squared_error: 0.1265 - val_loss: 0.1342 - val_mean_squared_error: 0.1342\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.13601 to 0.13418, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 94/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.1247 - mean_squared_error: 0.1247 - val_loss: 0.1324 - val_mean_squared_error: 0.1324\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.13418 to 0.13237, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 95/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.1230 - mean_squared_error: 0.1230 - val_loss: 0.1306 - val_mean_squared_error: 0.1306\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.13237 to 0.13059, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 96/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.1213 - mean_squared_error: 0.1213 - val_loss: 0.1288 - val_mean_squared_error: 0.1288\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.13059 to 0.12883, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 97/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.1196 - mean_squared_error: 0.1196 - val_loss: 0.1271 - val_mean_squared_error: 0.1271\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.12883 to 0.12710, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 98/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.1179 - mean_squared_error: 0.1179 - val_loss: 0.1254 - val_mean_squared_error: 0.1254\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.12710 to 0.12539, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 99/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.1162 - mean_squared_error: 0.1162 - val_loss: 0.1237 - val_mean_squared_error: 0.1237\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.12539 to 0.12370, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 100/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.1146 - mean_squared_error: 0.1146 - val_loss: 0.1220 - val_mean_squared_error: 0.1220\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.12370 to 0.12204, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 101/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.1130 - mean_squared_error: 0.1130 - val_loss: 0.1204 - val_mean_squared_error: 0.1204\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.12204 to 0.12040, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 102/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.1114 - mean_squared_error: 0.1114 - val_loss: 0.1188 - val_mean_squared_error: 0.1188\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.12040 to 0.11878, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 103/200\n",
      "1140/1140 [==============================] - 0s 14us/step - loss: 0.1099 - mean_squared_error: 0.1099 - val_loss: 0.1172 - val_mean_squared_error: 0.1172\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.11878 to 0.11719, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 104/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1083 - mean_squared_error: 0.1083 - val_loss: 0.1156 - val_mean_squared_error: 0.1156\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.11719 to 0.11562, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 105/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1068 - mean_squared_error: 0.1068 - val_loss: 0.1141 - val_mean_squared_error: 0.1141\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.11562 to 0.11407, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 106/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1053 - mean_squared_error: 0.1053 - val_loss: 0.1125 - val_mean_squared_error: 0.1125\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.11407 to 0.11254, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 107/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1038 - mean_squared_error: 0.1038 - val_loss: 0.1110 - val_mean_squared_error: 0.1110\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.11254 to 0.11104, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 108/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.1024 - mean_squared_error: 0.1024 - val_loss: 0.1096 - val_mean_squared_error: 0.1096\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.11104 to 0.10956, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 109/200\n",
      "1140/1140 [==============================] - 0s 14us/step - loss: 0.1010 - mean_squared_error: 0.1010 - val_loss: 0.1081 - val_mean_squared_error: 0.1081\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.10956 to 0.10810, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 110/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0996 - mean_squared_error: 0.0996 - val_loss: 0.1067 - val_mean_squared_error: 0.1067\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.10810 to 0.10666, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 111/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0982 - mean_squared_error: 0.0982 - val_loss: 0.1052 - val_mean_squared_error: 0.1052\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.10666 to 0.10524, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 112/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0968 - mean_squared_error: 0.0968 - val_loss: 0.1038 - val_mean_squared_error: 0.1038\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.10524 to 0.10384, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 113/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0955 - mean_squared_error: 0.0955 - val_loss: 0.1025 - val_mean_squared_error: 0.1025\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.10384 to 0.10246, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 114/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0941 - mean_squared_error: 0.0941 - val_loss: 0.1011 - val_mean_squared_error: 0.1011\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.10246 to 0.10110, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 115/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0928 - mean_squared_error: 0.0928 - val_loss: 0.0998 - val_mean_squared_error: 0.0998\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.10110 to 0.09977, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 116/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0915 - mean_squared_error: 0.0915 - val_loss: 0.0984 - val_mean_squared_error: 0.0984\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.09977 to 0.09845, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 117/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0903 - mean_squared_error: 0.0903 - val_loss: 0.0972 - val_mean_squared_error: 0.0972\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.09845 to 0.09715, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 118/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0890 - mean_squared_error: 0.0890 - val_loss: 0.0959 - val_mean_squared_error: 0.0959\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.09715 to 0.09587, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 119/200\n",
      "1140/1140 [==============================] - 0s 21us/step - loss: 0.0878 - mean_squared_error: 0.0878 - val_loss: 0.0946 - val_mean_squared_error: 0.0946\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.09587 to 0.09461, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 120/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.0866 - mean_squared_error: 0.0866 - val_loss: 0.0934 - val_mean_squared_error: 0.0934\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.09461 to 0.09337, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 121/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0854 - mean_squared_error: 0.0854 - val_loss: 0.0922 - val_mean_squared_error: 0.0922\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.09337 to 0.09215, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 122/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0842 - mean_squared_error: 0.0842 - val_loss: 0.0910 - val_mean_squared_error: 0.0910\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.09215 to 0.09095, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 123/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0831 - mean_squared_error: 0.0831 - val_loss: 0.0898 - val_mean_squared_error: 0.0898\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.09095 to 0.08977, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 124/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0820 - mean_squared_error: 0.0820 - val_loss: 0.0886 - val_mean_squared_error: 0.0886\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.08977 to 0.08860, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 125/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0808 - mean_squared_error: 0.0808 - val_loss: 0.0875 - val_mean_squared_error: 0.0875\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.08860 to 0.08745, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 126/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0797 - mean_squared_error: 0.0797 - val_loss: 0.0863 - val_mean_squared_error: 0.0863\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.08745 to 0.08632, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 127/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0787 - mean_squared_error: 0.0787 - val_loss: 0.0852 - val_mean_squared_error: 0.0852\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.08632 to 0.08521, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 128/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0776 - mean_squared_error: 0.0776 - val_loss: 0.0841 - val_mean_squared_error: 0.0841\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.08521 to 0.08412, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 129/200\n",
      "1140/1140 [==============================] - 0s 14us/step - loss: 0.0766 - mean_squared_error: 0.0766 - val_loss: 0.0830 - val_mean_squared_error: 0.0830\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.08412 to 0.08304, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 130/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0755 - mean_squared_error: 0.0755 - val_loss: 0.0820 - val_mean_squared_error: 0.0820\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.08304 to 0.08198, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 131/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0745 - mean_squared_error: 0.0745 - val_loss: 0.0809 - val_mean_squared_error: 0.0809\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.08198 to 0.08094, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 132/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0735 - mean_squared_error: 0.0735 - val_loss: 0.0799 - val_mean_squared_error: 0.0799\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.08094 to 0.07991, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 133/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0725 - mean_squared_error: 0.0725 - val_loss: 0.0789 - val_mean_squared_error: 0.0789\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.07991 to 0.07890, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 134/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0716 - mean_squared_error: 0.0716 - val_loss: 0.0779 - val_mean_squared_error: 0.0779\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.07890 to 0.07790, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 135/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0706 - mean_squared_error: 0.0706 - val_loss: 0.0769 - val_mean_squared_error: 0.0769\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.07790 to 0.07692, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 136/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0697 - mean_squared_error: 0.0697 - val_loss: 0.0760 - val_mean_squared_error: 0.0760\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.07692 to 0.07596, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 137/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0688 - mean_squared_error: 0.0688 - val_loss: 0.0750 - val_mean_squared_error: 0.0750\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.07596 to 0.07501, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 138/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0679 - mean_squared_error: 0.0679 - val_loss: 0.0741 - val_mean_squared_error: 0.0741\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.07501 to 0.07408, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 139/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0670 - mean_squared_error: 0.0670 - val_loss: 0.0732 - val_mean_squared_error: 0.0732\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.07408 to 0.07317, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 140/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0661 - mean_squared_error: 0.0661 - val_loss: 0.0723 - val_mean_squared_error: 0.0723\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.07317 to 0.07226, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 141/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0652 - mean_squared_error: 0.0652 - val_loss: 0.0714 - val_mean_squared_error: 0.0714\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.07226 to 0.07138, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 142/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0644 - mean_squared_error: 0.0644 - val_loss: 0.0705 - val_mean_squared_error: 0.0705\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.07138 to 0.07051, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 143/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0636 - mean_squared_error: 0.0636 - val_loss: 0.0696 - val_mean_squared_error: 0.0696\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.07051 to 0.06965, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 144/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0628 - mean_squared_error: 0.0628 - val_loss: 0.0688 - val_mean_squared_error: 0.0688\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.06965 to 0.06880, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 145/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0620 - mean_squared_error: 0.0620 - val_loss: 0.0680 - val_mean_squared_error: 0.0680\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.06880 to 0.06798, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 146/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0612 - mean_squared_error: 0.0612 - val_loss: 0.0672 - val_mean_squared_error: 0.0672\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.06798 to 0.06716, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 147/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0604 - mean_squared_error: 0.0604 - val_loss: 0.0664 - val_mean_squared_error: 0.0664\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.06716 to 0.06636, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 148/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0596 - mean_squared_error: 0.0596 - val_loss: 0.0656 - val_mean_squared_error: 0.0656\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.06636 to 0.06557, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 149/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0589 - mean_squared_error: 0.0589 - val_loss: 0.0648 - val_mean_squared_error: 0.0648\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.06557 to 0.06480, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 150/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0582 - mean_squared_error: 0.0582 - val_loss: 0.0640 - val_mean_squared_error: 0.0640\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.06480 to 0.06404, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 151/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0574 - mean_squared_error: 0.0574 - val_loss: 0.0633 - val_mean_squared_error: 0.0633\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.06404 to 0.06329, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 152/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0567 - mean_squared_error: 0.0567 - val_loss: 0.0626 - val_mean_squared_error: 0.0626\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.06329 to 0.06255, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 153/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0560 - mean_squared_error: 0.0560 - val_loss: 0.0618 - val_mean_squared_error: 0.0618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00153: val_loss improved from 0.06255 to 0.06183, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 154/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0554 - mean_squared_error: 0.0554 - val_loss: 0.0611 - val_mean_squared_error: 0.0611\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.06183 to 0.06112, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 155/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0547 - mean_squared_error: 0.0547 - val_loss: 0.0604 - val_mean_squared_error: 0.0604\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.06112 to 0.06042, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 156/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0540 - mean_squared_error: 0.0540 - val_loss: 0.0597 - val_mean_squared_error: 0.0597\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.06042 to 0.05974, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 157/200\n",
      "1140/1140 [==============================] - 0s 20us/step - loss: 0.0534 - mean_squared_error: 0.0534 - val_loss: 0.0591 - val_mean_squared_error: 0.0591\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.05974 to 0.05906, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 158/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0527 - mean_squared_error: 0.0527 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.05906 to 0.05840, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 159/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0521 - mean_squared_error: 0.0521 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.05840 to 0.05775, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 160/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0515 - mean_squared_error: 0.0515 - val_loss: 0.0571 - val_mean_squared_error: 0.0571\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.05775 to 0.05711, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 161/200\n",
      "1140/1140 [==============================] - 0s 14us/step - loss: 0.0509 - mean_squared_error: 0.0509 - val_loss: 0.0565 - val_mean_squared_error: 0.0565\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.05711 to 0.05649, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 162/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0503 - mean_squared_error: 0.0503 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.05649 to 0.05587, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 163/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0497 - mean_squared_error: 0.0497 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.05587 to 0.05527, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 164/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0492 - mean_squared_error: 0.0492 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.05527 to 0.05467, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 165/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.05467 to 0.05409, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 166/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0481 - mean_squared_error: 0.0481 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.05409 to 0.05352, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 167/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0475 - mean_squared_error: 0.0475 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.05352 to 0.05296, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 168/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0470 - mean_squared_error: 0.0470 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.05296 to 0.05241, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 169/200\n",
      "1140/1140 [==============================] - 0s 23us/step - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.05241 to 0.05186, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 170/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.05186 to 0.05133, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 171/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.05133 to 0.05081, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 172/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0450 - mean_squared_error: 0.0450 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.05081 to 0.05030, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 173/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.05030 to 0.04980, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 174/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.04980 to 0.04930, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 175/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.04930 to 0.04882, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 176/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0431 - mean_squared_error: 0.0431 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.04882 to 0.04835, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 177/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.04835 to 0.04788, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 178/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.04788 to 0.04742, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 179/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.04742 to 0.04698, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 180/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.04698 to 0.04654, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 181/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.04654 to 0.04611, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 182/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.04611 to 0.04568, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 183/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.04568 to 0.04527, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 184/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.04527 to 0.04486, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 185/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0395 - mean_squared_error: 0.0395 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.04486 to 0.04447, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 186/200\n",
      "1140/1140 [==============================] - 0s 19us/step - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.04447 to 0.04408, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 187/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.04408 to 0.04369, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 188/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.04369 to 0.04332, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 189/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.04332 to 0.04295, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 190/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.04295 to 0.04259, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 191/200\n",
      "1140/1140 [==============================] - 0s 16us/step - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.04259 to 0.04224, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 192/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.04224 to 0.04189, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 193/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.04189 to 0.04155, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 194/200\n",
      "1140/1140 [==============================] - 0s 15us/step - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.04155 to 0.04122, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 195/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.04122 to 0.04089, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 196/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.04089 to 0.04057, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 197/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.04057 to 0.04026, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 198/200\n",
      "1140/1140 [==============================] - 0s 17us/step - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.04026 to 0.03995, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 199/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.03995 to 0.03966, saving model to ../models/best_mlp_model.h5\n",
      "Epoch 200/200\n",
      "1140/1140 [==============================] - 0s 18us/step - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.03966 to 0.03936, saving model to ../models/best_mlp_model.h5\n",
      "Done..!\n"
     ]
    }
   ],
   "source": [
    "model = build_keras_model()\n",
    "\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "\n",
    "# save the best model only\n",
    "mc = ModelCheckpoint('../models/best_mlp_model.h5'\n",
    "                     , monitor='val_loss'\n",
    "                     , mode='min' \n",
    "                     , verbose=1\n",
    "                     #, save_weights_only=True\n",
    "                     , save_best_only=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)\n",
    "\n",
    "callbacks = [es, mc, reduce_lr]\n",
    "\n",
    "history = model.fit(x_train, y_train\n",
    "                    , epochs=epochs\n",
    "                    , batch_size=batch_size\n",
    "                    , verbose=1\n",
    "                    , validation_split=validation_split\n",
    "                    , shuffle=False\n",
    "                    , callbacks=callbacks)\n",
    "\n",
    "print (\"Done..!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Validation MSE mean: 0.16  std: (0.11)\n"
     ]
    }
   ],
   "source": [
    "val_loss = history.history['val_loss']\n",
    "val_loss_df = pd.DataFrame(val_loss)\n",
    "\n",
    "print(\"Model Validation MSE mean: %.2f  std: (%.2f)\" % \\\n",
    "      (val_loss_df.mean(), val_loss_df.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.015, Test MSE: 0.012\n"
     ]
    }
   ],
   "source": [
    "# load the saved model\n",
    "saved_model = load_model('../models/best_mlp_model.h5')\n",
    "\n",
    "# evaluate the model\n",
    "train_loss, train_mse = saved_model.evaluate(x_train, y_train, verbose=0)\n",
    "test_loss, test_mse = saved_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Train MSE: %.3f, Test MSE: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53463936],\n",
       "       [0.53463936],\n",
       "       [0.53463936],\n",
       "       ...,\n",
       "       [0.53463936],\n",
       "       [0.53463936],\n",
       "       [0.53463936]], dtype=float32)"
      ]
     },
     "execution_count": 862,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regressor.fit(x_train, y_train)\n",
    "y_hat = saved_model.predict(x_train)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 (training) = -0.44431412825789196\n"
     ]
    }
   ],
   "source": [
    "r2_train = metrics.r2_score(y_train, y_hat)\n",
    "print('R2 (training) = {}'.format(r2_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_train, y_hat)\n",
    "plt.xlabel('Latency', size=18)\n",
    "plt.ylabel(\"predicted Latency\", size=18)\n",
    "#plt.xlim(-2,3)\n",
    "#plt.ylim(-3,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Residual PDF')"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train = y_train.values\n",
    "sns.distplot(y_train - y_hat)\n",
    "plt.title('Residual PDF', size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Residual mean: 0.10 std: 0.16  min; -0.26 max: 0.47\n"
     ]
    }
   ],
   "source": [
    "# evaluation mean_absolute_percentage_error\n",
    "train_error =  y_train - y_hat\n",
    "train_error\n",
    "\n",
    "mean_error = np.mean(train_error)\n",
    "min_error = np.min(train_error)\n",
    "max_error = np.max(train_error)\n",
    "std_error = np.std(train_error)\n",
    "\n",
    "print(\"Train Residual mean: %.2f std: %.2f  min; %.2f max: %.2f\" \\\n",
    "      % (mean_error, std_error, min_error, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = saved_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test, y_hat_test)\n",
    "plt.xlabel('Latency', size=18)\n",
    "plt.ylabel('Predicted Latency', size=18)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.707</td>\n",
       "      <td>2.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prediction  target\n",
       "0       1.707   1.785\n",
       "1       1.707   1.796\n",
       "2       1.707   1.697\n",
       "3       1.707   2.298\n",
       "4       1.707   1.838"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf = pd.DataFrame(np.exp(y_hat_test), columns=['prediction'])\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "perf['target'] = np.exp(y_test)\n",
    "perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>residual</th>\n",
       "      <th>difference%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.785</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>4.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.796</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>4.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.697</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.707</td>\n",
       "      <td>2.298</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>25.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.838</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>7.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.707</td>\n",
       "      <td>2.269</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>24.777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.506</td>\n",
       "      <td>0.201</td>\n",
       "      <td>13.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.707</td>\n",
       "      <td>2.043</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>16.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.891</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>9.717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.707</td>\n",
       "      <td>2.230</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>23.468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  target  residual  difference%\n",
       "0         1.707   1.785    -0.078        4.374\n",
       "1         1.707   1.796    -0.089        4.946\n",
       "2         1.707   1.697     0.010        0.577\n",
       "3         1.707   2.298    -0.591       25.726\n",
       "4         1.707   1.838    -0.131        7.147\n",
       "..          ...     ...       ...          ...\n",
       "154       1.707   2.269    -0.562       24.777\n",
       "155       1.707   1.506     0.201       13.311\n",
       "156       1.707   2.043    -0.336       16.460\n",
       "157       1.707   1.891    -0.184        9.717\n",
       "158       1.707   2.230    -0.523       23.468\n",
       "\n",
       "[159 rows x 4 columns]"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing mean_absolute_percentage_error\n",
    "perf['residual'] = perf['prediction'] - perf['target']\n",
    "perf['difference%'] = np.absolute(perf['residual'] * 100 / perf['target'])\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>residual</th>\n",
       "      <th>difference%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159.000</td>\n",
       "      <td>159.000</td>\n",
       "      <td>159.000</td>\n",
       "      <td>159.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.925</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>14.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.305</td>\n",
       "      <td>8.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.467</td>\n",
       "      <td>-1.011</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.697</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>9.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.707</td>\n",
       "      <td>1.922</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>13.474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.707</td>\n",
       "      <td>2.177</td>\n",
       "      <td>0.010</td>\n",
       "      <td>21.581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.707</td>\n",
       "      <td>2.718</td>\n",
       "      <td>0.240</td>\n",
       "      <td>37.209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction  target  residual  difference%\n",
       "count     159.000 159.000   159.000      159.000\n",
       "mean        1.707   1.925    -0.219       14.793\n",
       "std         0.000   0.305     0.305        8.557\n",
       "min         1.707   1.467    -1.011        0.082\n",
       "25%         1.707   1.697    -0.470        9.468\n",
       "50%         1.707   1.922    -0.215       13.474\n",
       "75%         1.707   2.177     0.010       21.581\n",
       "max         1.707   2.718     0.240       37.209"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>residual</th>\n",
       "      <th>difference%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.71</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.72</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.72</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.72</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.73</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.73</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.76</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.76</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>5.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>5.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>5.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>5.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.84</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>7.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.13</td>\n",
       "      <td>7.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.87</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>8.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.88</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>9.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.89</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>9.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.90</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>10.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.90</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>10.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.90</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>10.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.91</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>10.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.91</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>10.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>11.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>11.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.94</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>12.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.94</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>12.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.94</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>12.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.19</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>13.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>13.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>13.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.97</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>13.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.97</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>13.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>14.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>14.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>14.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>15.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>15.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>15.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>15.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>15.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.03</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>16.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.71</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.24</td>\n",
       "      <td>16.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.04</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>16.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.04</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>16.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.05</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>16.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>17.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>17.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>17.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>17.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.09</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>18.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.09</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>18.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.09</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>18.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>18.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.11</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>19.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.11</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>19.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>19.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>20.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>20.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>20.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.17</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>21.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.17</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>21.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>21.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>21.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>22.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>22.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>22.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>22.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>22.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>23.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>23.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>23.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>23.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.23</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>23.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.23</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>23.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.23</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>23.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.23</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>23.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>23.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>23.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.25</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>24.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.26</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>24.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>24.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>24.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>24.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>25.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>25.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>25.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>25.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>25.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>25.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>25.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>26.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.35</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>27.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>30.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>30.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.46</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>30.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>31.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.52</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>32.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>33.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>35.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>36.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.71</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>37.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  target  residual  difference%\n",
       "9          1.71    1.71     -0.00         0.08\n",
       "37         1.71    1.70      0.01         0.58\n",
       "2          1.71    1.70      0.01         0.58\n",
       "117        1.71    1.70      0.01         0.58\n",
       "49         1.71    1.70      0.01         0.58\n",
       "68         1.71    1.72     -0.01         0.73\n",
       "142        1.71    1.72     -0.01         0.73\n",
       "51         1.71    1.72     -0.01         0.73\n",
       "20         1.71    1.69      0.02         1.25\n",
       "55         1.71    1.73     -0.02         1.36\n",
       "153        1.71    1.73     -0.02         1.36\n",
       "82         1.71    1.67      0.03         1.93\n",
       "67         1.71    1.74     -0.03         1.99\n",
       "5          1.71    1.75     -0.05         2.60\n",
       "43         1.71    1.75     -0.05         2.60\n",
       "77         1.71    1.76     -0.06         3.20\n",
       "40         1.71    1.76     -0.06         3.20\n",
       "38         1.71    1.77     -0.07         3.79\n",
       "53         1.71    1.77     -0.07         3.79\n",
       "107        1.71    1.77     -0.07         3.79\n",
       "115        1.71    1.77     -0.07         3.79\n",
       "99         1.71    1.77     -0.07         3.79\n",
       "56         1.71    1.78     -0.08         4.37\n",
       "91         1.71    1.78     -0.08         4.37\n",
       "0          1.71    1.78     -0.08         4.37\n",
       "136        1.71    1.78     -0.08         4.37\n",
       "135        1.71    1.78     -0.08         4.37\n",
       "12         1.71    1.80     -0.09         4.95\n",
       "65         1.71    1.80     -0.09         4.95\n",
       "1          1.71    1.80     -0.09         4.95\n",
       "131        1.71    1.81     -0.10         5.51\n",
       "92         1.71    1.81     -0.10         5.51\n",
       "120        1.71    1.81     -0.10         5.51\n",
       "63         1.71    1.81     -0.10         5.51\n",
       "4          1.71    1.84     -0.13         7.15\n",
       "45         1.71    1.58      0.13         7.96\n",
       "93         1.71    1.87     -0.16         8.71\n",
       "13         1.71    1.57      0.14         8.80\n",
       "57         1.71    1.57      0.14         8.80\n",
       "60         1.71    1.88     -0.17         9.22\n",
       "157        1.71    1.89     -0.18         9.72\n",
       "83         1.71    1.90     -0.19        10.21\n",
       "23         1.71    1.90     -0.19        10.21\n",
       "50         1.71    1.90     -0.19        10.21\n",
       "98         1.71    1.54      0.16        10.53\n",
       "119        1.71    1.91     -0.20        10.69\n",
       "28         1.71    1.91     -0.20        10.69\n",
       "73         1.71    1.92     -0.21        11.17\n",
       "147        1.71    1.92     -0.21        11.17\n",
       "134        1.71    1.94     -0.24        12.11\n",
       "114        1.71    1.94     -0.24        12.11\n",
       "118        1.71    1.94     -0.24        12.11\n",
       "16         1.71    1.52      0.19        12.36\n",
       "78         1.71    1.52      0.19        12.36\n",
       "27         1.71    1.52      0.19        12.36\n",
       "29         1.71    1.52      0.19        12.36\n",
       "89         1.71    1.52      0.19        12.36\n",
       "35         1.71    1.52      0.19        12.36\n",
       "6          1.71    1.52      0.19        12.36\n",
       "41         1.71    1.52      0.19        12.36\n",
       "8          1.71    1.52      0.19        12.36\n",
       "36         1.71    1.52      0.19        12.36\n",
       "52         1.71    1.52      0.19        12.36\n",
       "102        1.71    1.52      0.19        12.36\n",
       "132        1.71    1.96     -0.26        13.03\n",
       "17         1.71    1.96     -0.26        13.03\n",
       "128        1.71    1.96     -0.26        13.03\n",
       "54         1.71    1.51      0.20        13.31\n",
       "86         1.71    1.51      0.20        13.31\n",
       "155        1.71    1.51      0.20        13.31\n",
       "71         1.71    1.51      0.20        13.31\n",
       "121        1.71    1.51      0.20        13.31\n",
       "151        1.71    1.51      0.20        13.31\n",
       "18         1.71    1.51      0.20        13.31\n",
       "66         1.71    1.51      0.20        13.31\n",
       "14         1.71    1.51      0.20        13.31\n",
       "21         1.71    1.51      0.20        13.31\n",
       "24         1.71    1.51      0.20        13.31\n",
       "125        1.71    1.51      0.20        13.31\n",
       "104        1.71    1.97     -0.27        13.47\n",
       "81         1.71    1.97     -0.27        13.47\n",
       "95         1.71    1.49      0.21        14.29\n",
       "79         1.71    1.49      0.21        14.29\n",
       "34         1.71    1.49      0.21        14.29\n",
       "7          1.71    1.49      0.21        14.29\n",
       "69         1.71    1.49      0.21        14.29\n",
       "129        1.71    1.99     -0.29        14.35\n",
       "105        1.71    1.99     -0.29        14.35\n",
       "94         1.71    1.99     -0.29        14.35\n",
       "10         1.71    2.01     -0.31        15.21\n",
       "87         1.71    2.01     -0.31        15.21\n",
       "126        1.71    2.01     -0.31        15.21\n",
       "111        1.71    2.01     -0.31        15.21\n",
       "19         1.71    1.48      0.23        15.30\n",
       "74         1.71    2.03     -0.33        16.05\n",
       "76         1.71    1.47      0.24        16.35\n",
       "156        1.71    2.04     -0.34        16.46\n",
       "137        1.71    2.04     -0.34        16.46\n",
       "127        1.71    2.05     -0.35        16.87\n",
       "138        1.71    2.06     -0.36        17.27\n",
       "152        1.71    2.07     -0.37        17.67\n",
       "47         1.71    2.07     -0.37        17.67\n",
       "96         1.71    2.07     -0.37        17.67\n",
       "85         1.71    2.08     -0.38        18.06\n",
       "22         1.71    2.08     -0.38        18.06\n",
       "108        1.71    2.08     -0.38        18.06\n",
       "143        1.71    2.08     -0.38        18.06\n",
       "113        1.71    2.09     -0.39        18.45\n",
       "109        1.71    2.09     -0.39        18.45\n",
       "116        1.71    2.09     -0.39        18.45\n",
       "145        1.71    2.10     -0.40        18.83\n",
       "75         1.71    2.11     -0.41        19.21\n",
       "141        1.71    2.11     -0.41        19.21\n",
       "112        1.71    2.13     -0.43        19.96\n",
       "62         1.71    2.14     -0.44        20.32\n",
       "100        1.71    2.15     -0.45        20.69\n",
       "31         1.71    2.15     -0.45        20.69\n",
       "80         1.71    2.17     -0.46        21.40\n",
       "88         1.71    2.17     -0.46        21.40\n",
       "149        1.71    2.18     -0.47        21.76\n",
       "103        1.71    2.18     -0.47        21.76\n",
       "110        1.71    2.19     -0.48        22.11\n",
       "148        1.71    2.19     -0.48        22.11\n",
       "33         1.71    2.19     -0.48        22.11\n",
       "139        1.71    2.21     -0.50        22.79\n",
       "130        1.71    2.21     -0.50        22.79\n",
       "70         1.71    2.22     -0.51        23.13\n",
       "150        1.71    2.22     -0.51        23.13\n",
       "48         1.71    2.22     -0.51        23.13\n",
       "101        1.71    2.22     -0.51        23.13\n",
       "158        1.71    2.23     -0.52        23.47\n",
       "97         1.71    2.23     -0.52        23.47\n",
       "144        1.71    2.23     -0.52        23.47\n",
       "32         1.71    2.23     -0.52        23.47\n",
       "124        1.71    2.24     -0.53        23.80\n",
       "26         1.71    2.24     -0.53        23.80\n",
       "146        1.71    2.25     -0.54        24.13\n",
       "72         1.71    2.26     -0.55        24.45\n",
       "154        1.71    2.27     -0.56        24.78\n",
       "58         1.71    2.27     -0.56        24.78\n",
       "15         1.71    2.27     -0.56        24.78\n",
       "106        1.71    2.28     -0.57        25.10\n",
       "39         1.71    2.28     -0.57        25.10\n",
       "122        1.71    2.28     -0.57        25.10\n",
       "84         1.71    2.28     -0.57        25.10\n",
       "61         1.71    2.30     -0.59        25.73\n",
       "3          1.71    2.30     -0.59        25.73\n",
       "140        1.71    2.30     -0.59        25.73\n",
       "44         1.71    2.33     -0.62        26.65\n",
       "133        1.71    2.35     -0.64        27.25\n",
       "11         1.71    2.45     -0.74        30.38\n",
       "59         1.71    2.45     -0.74        30.38\n",
       "90         1.71    2.46     -0.75        30.65\n",
       "64         1.71    2.48     -0.77        31.19\n",
       "42         1.71    2.52     -0.81        32.23\n",
       "123        1.71    2.59     -0.88        33.98\n",
       "46         1.71    2.65     -0.94        35.63\n",
       "30         1.71    2.70     -0.99        36.77\n",
       "25         1.71    2.72     -1.01        37.21"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "perf.sort_values(by = ['difference%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 (testing) = -0.46298537393657835\n"
     ]
    }
   ],
   "source": [
    "r2_test = metrics.r2_score(y_test, y_hat_test)\n",
    "print('R2 (testing) = {}'.format(r2_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
